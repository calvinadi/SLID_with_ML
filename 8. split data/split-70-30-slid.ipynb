{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8848655,"sourceType":"datasetVersion","datasetId":5326116}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-06T10:35:30.691748Z","iopub.execute_input":"2024-07-06T10:35:30.692150Z","iopub.status.idle":"2024-07-06T10:35:31.629140Z","shell.execute_reply.started":"2024-07-06T10:35:30.692113Z","shell.execute_reply":"2024-07-06T10:35:31.627964Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nimport joblib\nfrom sklearn.model_selection import KFold","metadata":{"execution":{"iopub.status.busy":"2024-07-06T10:35:31.635325Z","iopub.execute_input":"2024-07-06T10:35:31.635627Z","iopub.status.idle":"2024-07-06T10:35:31.640882Z","shell.execute_reply.started":"2024-07-06T10:35:31.635600Z","shell.execute_reply":"2024-07-06T10:35:31.639868Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(r\"/kaggle/input/feature-common-language/audio_features_partial.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T10:35:31.642081Z","iopub.execute_input":"2024-07-06T10:35:31.642387Z","iopub.status.idle":"2024-07-06T10:35:32.615478Z","shell.execute_reply.started":"2024-07-06T10:35:31.642358Z","shell.execute_reply":"2024-07-06T10:35:32.614474Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   chroma_stft       rms  spectral_centroid  spectral_bandwidth  \\\n0     0.712224  0.055740        2989.050985         2193.800068   \n1     0.598403  0.074004        2372.315827         2065.561020   \n2     0.756316  0.046051        3274.178654         2196.474265   \n3     0.588983  0.061376        1948.418292         2049.242741   \n4     0.647222  0.069638        1705.618989         1824.714129   \n\n   spectral_rolloff  zero_crossing_rate     mfcc_1     mfcc_2     mfcc_3  \\\n0       5666.294643            0.275144 -202.32579  27.614292   4.094784   \n1       4795.649510            0.178041 -137.41476  59.931843   5.077963   \n2       5870.572917            0.353054 -233.02031  22.691550  10.057923   \n3       4186.921296            0.137682 -199.04490  80.806870  31.451380   \n4       3730.709877            0.095540 -325.47556  92.373820  17.725632   \n\n      mfcc_4  ...    mfcc_13   mfcc_14    mfcc_15   mfcc_16   mfcc_17  \\\n0   5.301181  ...   4.752774 -0.985637  -6.752584 -4.679379 -5.478848   \n1  -5.712012  ... -12.469353 -2.134825 -10.989368 -1.460541 -4.485021   \n2   3.829097  ...  -1.657243 -5.409642  -4.017134 -6.744406 -1.697630   \n3  -1.297673  ...  -4.202263  0.065943  -9.312079 -7.164060 -3.082040   \n4  31.867613  ...   3.169903  4.538502  -1.073114 -1.204524 -0.108214   \n\n    mfcc_18   mfcc_19   mfcc_20   label  \\\n0 -0.866508 -1.919669 -0.634521  Arabic   \n1 -0.408789 -8.211143 -5.170048  Arabic   \n2 -0.387302  0.829549  1.292110  Arabic   \n3 -8.046175 -3.083879 -2.018449  Arabic   \n4 -4.803460 -2.882802 -1.455632  Arabic   \n\n                                           file_path  \n0  /kaggle/input/preprocess-common-language/proce...  \n1  /kaggle/input/preprocess-common-language/proce...  \n2  /kaggle/input/preprocess-common-language/proce...  \n3  /kaggle/input/preprocess-common-language/proce...  \n4  /kaggle/input/preprocess-common-language/proce...  \n\n[5 rows x 28 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chroma_stft</th>\n      <th>rms</th>\n      <th>spectral_centroid</th>\n      <th>spectral_bandwidth</th>\n      <th>spectral_rolloff</th>\n      <th>zero_crossing_rate</th>\n      <th>mfcc_1</th>\n      <th>mfcc_2</th>\n      <th>mfcc_3</th>\n      <th>mfcc_4</th>\n      <th>...</th>\n      <th>mfcc_13</th>\n      <th>mfcc_14</th>\n      <th>mfcc_15</th>\n      <th>mfcc_16</th>\n      <th>mfcc_17</th>\n      <th>mfcc_18</th>\n      <th>mfcc_19</th>\n      <th>mfcc_20</th>\n      <th>label</th>\n      <th>file_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.712224</td>\n      <td>0.055740</td>\n      <td>2989.050985</td>\n      <td>2193.800068</td>\n      <td>5666.294643</td>\n      <td>0.275144</td>\n      <td>-202.32579</td>\n      <td>27.614292</td>\n      <td>4.094784</td>\n      <td>5.301181</td>\n      <td>...</td>\n      <td>4.752774</td>\n      <td>-0.985637</td>\n      <td>-6.752584</td>\n      <td>-4.679379</td>\n      <td>-5.478848</td>\n      <td>-0.866508</td>\n      <td>-1.919669</td>\n      <td>-0.634521</td>\n      <td>Arabic</td>\n      <td>/kaggle/input/preprocess-common-language/proce...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.598403</td>\n      <td>0.074004</td>\n      <td>2372.315827</td>\n      <td>2065.561020</td>\n      <td>4795.649510</td>\n      <td>0.178041</td>\n      <td>-137.41476</td>\n      <td>59.931843</td>\n      <td>5.077963</td>\n      <td>-5.712012</td>\n      <td>...</td>\n      <td>-12.469353</td>\n      <td>-2.134825</td>\n      <td>-10.989368</td>\n      <td>-1.460541</td>\n      <td>-4.485021</td>\n      <td>-0.408789</td>\n      <td>-8.211143</td>\n      <td>-5.170048</td>\n      <td>Arabic</td>\n      <td>/kaggle/input/preprocess-common-language/proce...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.756316</td>\n      <td>0.046051</td>\n      <td>3274.178654</td>\n      <td>2196.474265</td>\n      <td>5870.572917</td>\n      <td>0.353054</td>\n      <td>-233.02031</td>\n      <td>22.691550</td>\n      <td>10.057923</td>\n      <td>3.829097</td>\n      <td>...</td>\n      <td>-1.657243</td>\n      <td>-5.409642</td>\n      <td>-4.017134</td>\n      <td>-6.744406</td>\n      <td>-1.697630</td>\n      <td>-0.387302</td>\n      <td>0.829549</td>\n      <td>1.292110</td>\n      <td>Arabic</td>\n      <td>/kaggle/input/preprocess-common-language/proce...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.588983</td>\n      <td>0.061376</td>\n      <td>1948.418292</td>\n      <td>2049.242741</td>\n      <td>4186.921296</td>\n      <td>0.137682</td>\n      <td>-199.04490</td>\n      <td>80.806870</td>\n      <td>31.451380</td>\n      <td>-1.297673</td>\n      <td>...</td>\n      <td>-4.202263</td>\n      <td>0.065943</td>\n      <td>-9.312079</td>\n      <td>-7.164060</td>\n      <td>-3.082040</td>\n      <td>-8.046175</td>\n      <td>-3.083879</td>\n      <td>-2.018449</td>\n      <td>Arabic</td>\n      <td>/kaggle/input/preprocess-common-language/proce...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.647222</td>\n      <td>0.069638</td>\n      <td>1705.618989</td>\n      <td>1824.714129</td>\n      <td>3730.709877</td>\n      <td>0.095540</td>\n      <td>-325.47556</td>\n      <td>92.373820</td>\n      <td>17.725632</td>\n      <td>31.867613</td>\n      <td>...</td>\n      <td>3.169903</td>\n      <td>4.538502</td>\n      <td>-1.073114</td>\n      <td>-1.204524</td>\n      <td>-0.108214</td>\n      <td>-4.803460</td>\n      <td>-2.882802</td>\n      <td>-1.455632</td>\n      <td>Arabic</td>\n      <td>/kaggle/input/preprocess-common-language/proce...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 28 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T10:35:32.617940Z","iopub.execute_input":"2024-07-06T10:35:32.618317Z","iopub.status.idle":"2024-07-06T10:35:32.853612Z","shell.execute_reply.started":"2024-07-06T10:35:32.618287Z","shell.execute_reply":"2024-07-06T10:35:32.852624Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Initialize LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Fit LabelEncoder with your actual labels\nlabel_encoder.fit(df['label'])\n\n# Transform actual labels to numeric labels\ndf['numeric_labels'] = label_encoder.transform(df['label'])","metadata":{"execution":{"iopub.status.busy":"2024-07-06T10:35:32.855034Z","iopub.execute_input":"2024-07-06T10:35:32.855460Z","iopub.status.idle":"2024-07-06T10:35:32.883354Z","shell.execute_reply.started":"2024-07-06T10:35:32.855422Z","shell.execute_reply":"2024-07-06T10:35:32.881993Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"X = df.drop(columns=['label','numeric_labels','file_path'])\ny = df['numeric_labels']","metadata":{"execution":{"iopub.status.busy":"2024-07-06T10:35:32.884644Z","iopub.execute_input":"2024-07-06T10:35:32.885013Z","iopub.status.idle":"2024-07-06T10:35:32.908076Z","shell.execute_reply.started":"2024-07-06T10:35:32.884977Z","shell.execute_reply":"2024-07-06T10:35:32.906927Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(sampling_strategy='auto', random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X, y)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T10:35:32.909354Z","iopub.execute_input":"2024-07-06T10:35:32.909664Z","iopub.status.idle":"2024-07-06T10:35:34.127958Z","shell.execute_reply.started":"2024-07-06T10:35:32.909638Z","shell.execute_reply":"2024-07-06T10:35:34.126779Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\n\n# Jumlah kelas sebelum SMOTE\nprint(\"Jumlah kelas sebelum SMOTE:\")\nprint(Counter(y))\n\n# Jumlah kelas setelah SMOTE\nprint(\"\\nJumlah kelas setelah SMOTE:\")\nprint(Counter(y_resampled))","metadata":{"execution":{"iopub.status.busy":"2024-07-06T10:35:34.129284Z","iopub.execute_input":"2024-07-06T10:35:34.129738Z","iopub.status.idle":"2024-07-06T10:35:34.172972Z","shell.execute_reply.started":"2024-07-06T10:35:34.129709Z","shell.execute_reply":"2024-07-06T10:35:34.171920Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Jumlah kelas sebelum SMOTE:\nCounter({2: 3570, 39: 3177, 19: 3012, 6: 3003, 27: 2955, 41: 2784, 24: 2766, 0: 2751, 20: 2733, 37: 2691, 10: 2550, 33: 2547, 18: 2520, 21: 2484, 42: 2472, 8: 2442, 40: 2367, 26: 2331, 30: 2283, 28: 2277, 23: 2268, 32: 2238, 7: 2202, 31: 2196, 43: 2181, 15: 2073, 44: 2073, 5: 2016, 34: 1977, 9: 1962, 25: 1947, 12: 1932, 1: 1914, 16: 1890, 29: 1884, 14: 1863, 3: 1830, 17: 1830, 4: 1797, 11: 1773, 35: 1758, 36: 1752, 38: 1737, 22: 1695, 13: 1623})\n\nJumlah kelas setelah SMOTE:\nCounter({0: 3570, 4: 3570, 9: 3570, 27: 3570, 18: 3570, 24: 3570, 23: 3570, 13: 3570, 20: 3570, 11: 3570, 16: 3570, 40: 3570, 28: 3570, 15: 3570, 21: 3570, 8: 3570, 31: 3570, 29: 3570, 5: 3570, 7: 3570, 41: 3570, 44: 3570, 3: 3570, 2: 3570, 14: 3570, 43: 3570, 36: 3570, 26: 3570, 34: 3570, 12: 3570, 37: 3570, 35: 3570, 42: 3570, 33: 3570, 17: 3570, 25: 3570, 6: 3570, 30: 3570, 39: 3570, 22: 3570, 10: 3570, 1: 3570, 19: 3570, 32: 3570, 38: 3570})\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled,\n                                                    test_size=0.3, random_state=42,\n                                                    stratify=y_resampled)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T10:35:34.174193Z","iopub.execute_input":"2024-07-06T10:35:34.174560Z","iopub.status.idle":"2024-07-06T10:35:34.325987Z","shell.execute_reply.started":"2024-07-06T10:35:34.174525Z","shell.execute_reply":"2024-07-06T10:35:34.325004Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Inisialisasi StandardScaler\nscaler = StandardScaler()\n\n# Fit scaler pada data training dan transform kedua set data\nX_train_normalized = scaler.fit_transform(X_train)\nX_test_normalized = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T10:35:34.327222Z","iopub.execute_input":"2024-07-06T10:35:34.327545Z","iopub.status.idle":"2024-07-06T10:35:34.407335Z","shell.execute_reply.started":"2024-07-06T10:35:34.327519Z","shell.execute_reply":"2024-07-06T10:35:34.406156Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(X_train_normalized)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T10:35:34.408833Z","iopub.execute_input":"2024-07-06T10:35:34.409839Z","iopub.status.idle":"2024-07-06T10:35:34.446648Z","shell.execute_reply.started":"2024-07-06T10:35:34.409798Z","shell.execute_reply":"2024-07-06T10:35:34.445620Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"              0         1         2         3         4         5         6   \\\n0      -0.364935 -0.247849 -0.890410  0.214314 -0.561075 -1.112044  0.618754   \n1      -1.561425 -0.344321 -0.135153  0.424011  0.131912 -0.623506  0.329402   \n2       0.665183 -0.612016  0.750516  0.902349  0.803726  0.496568  1.061305   \n3      -1.030974  2.093594 -1.470936 -1.994267 -1.551235 -1.101425 -0.378292   \n4       0.194780  1.141861 -0.860718 -1.013396 -1.069192 -0.552857 -0.181194   \n...          ...       ...       ...       ...       ...       ...       ...   \n112450 -0.133160 -0.669898  0.272018  0.602705  0.497571 -0.091507  0.424125   \n112451 -0.262421 -0.089078  0.276614 -0.028210  0.196182  0.112838  1.198047   \n112452  2.366706 -1.512846  1.166286  0.784500  0.992960  1.409006 -0.042223   \n112453 -0.996360 -0.528871  0.698901  0.757775  0.812551  0.348989  0.063959   \n112454 -0.599571 -0.446375 -0.200835  0.447156  0.057043 -0.589107  0.224776   \n\n              7         8         9   ...        16        17        18  \\\n0       0.807991  0.786938 -1.582598  ... -0.372540 -0.425312  0.083558   \n1      -0.386126 -0.340000 -0.284849  ... -0.261647 -0.655708 -0.219533   \n2      -0.548987  1.098359  0.939015  ...  1.132156  0.817791  0.687609   \n3       1.297652 -1.920052  3.791689  ... -1.754882  1.290520 -1.386529   \n4       1.020571 -1.213755 -0.296157  ... -0.063017 -0.148142 -1.784751   \n...          ...       ...       ...  ...       ...       ...       ...   \n112450 -0.346555  0.084319 -1.099335  ... -0.470875 -0.623940 -0.557836   \n112451 -0.642695 -0.807168  0.112235  ...  0.262296  0.962201 -1.422439   \n112452 -0.704344 -0.166065 -0.228819  ...  1.524984  0.901985  0.707583   \n112453 -0.708739  1.677042  0.055551  ... -0.501684 -0.195061  0.241821   \n112454  0.027139  0.389206  0.462315  ... -0.072461 -0.636520  0.757840   \n\n              19        20        21        22        23        24        25  \n0       0.188499  0.269625 -0.357414 -0.257548 -0.406506 -0.116368 -0.662683  \n1      -0.482159 -1.300348 -0.336036 -0.162236 -1.568699 -0.613665 -0.441593  \n2      -0.440398  0.416434  0.405453  1.682787  1.369895  0.290245 -1.050559  \n3      -1.329693  0.619006  0.477908 -0.643397 -0.029769  0.436081 -0.172504  \n4       1.201975 -1.022463  0.221809 -0.767619  0.941025 -1.583400  2.624626  \n...          ...       ...       ...       ...       ...       ...       ...  \n112450 -0.275097 -0.082015 -0.352531  0.565829 -0.353458 -0.051545  0.009822  \n112451 -1.717809  0.246690 -0.858461  0.601352  0.470246  0.015822  0.495168  \n112452  0.547474  0.926508  0.417688  1.032225  1.036761  0.784477  0.441887  \n112453 -0.083057 -0.283776  0.640558 -0.922673 -0.655744 -0.824914 -1.049133  \n112454  0.872088  1.752921  1.078445  0.578571 -0.126062 -0.437330  1.209496  \n\n[112455 rows x 26 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.364935</td>\n      <td>-0.247849</td>\n      <td>-0.890410</td>\n      <td>0.214314</td>\n      <td>-0.561075</td>\n      <td>-1.112044</td>\n      <td>0.618754</td>\n      <td>0.807991</td>\n      <td>0.786938</td>\n      <td>-1.582598</td>\n      <td>...</td>\n      <td>-0.372540</td>\n      <td>-0.425312</td>\n      <td>0.083558</td>\n      <td>0.188499</td>\n      <td>0.269625</td>\n      <td>-0.357414</td>\n      <td>-0.257548</td>\n      <td>-0.406506</td>\n      <td>-0.116368</td>\n      <td>-0.662683</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.561425</td>\n      <td>-0.344321</td>\n      <td>-0.135153</td>\n      <td>0.424011</td>\n      <td>0.131912</td>\n      <td>-0.623506</td>\n      <td>0.329402</td>\n      <td>-0.386126</td>\n      <td>-0.340000</td>\n      <td>-0.284849</td>\n      <td>...</td>\n      <td>-0.261647</td>\n      <td>-0.655708</td>\n      <td>-0.219533</td>\n      <td>-0.482159</td>\n      <td>-1.300348</td>\n      <td>-0.336036</td>\n      <td>-0.162236</td>\n      <td>-1.568699</td>\n      <td>-0.613665</td>\n      <td>-0.441593</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.665183</td>\n      <td>-0.612016</td>\n      <td>0.750516</td>\n      <td>0.902349</td>\n      <td>0.803726</td>\n      <td>0.496568</td>\n      <td>1.061305</td>\n      <td>-0.548987</td>\n      <td>1.098359</td>\n      <td>0.939015</td>\n      <td>...</td>\n      <td>1.132156</td>\n      <td>0.817791</td>\n      <td>0.687609</td>\n      <td>-0.440398</td>\n      <td>0.416434</td>\n      <td>0.405453</td>\n      <td>1.682787</td>\n      <td>1.369895</td>\n      <td>0.290245</td>\n      <td>-1.050559</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1.030974</td>\n      <td>2.093594</td>\n      <td>-1.470936</td>\n      <td>-1.994267</td>\n      <td>-1.551235</td>\n      <td>-1.101425</td>\n      <td>-0.378292</td>\n      <td>1.297652</td>\n      <td>-1.920052</td>\n      <td>3.791689</td>\n      <td>...</td>\n      <td>-1.754882</td>\n      <td>1.290520</td>\n      <td>-1.386529</td>\n      <td>-1.329693</td>\n      <td>0.619006</td>\n      <td>0.477908</td>\n      <td>-0.643397</td>\n      <td>-0.029769</td>\n      <td>0.436081</td>\n      <td>-0.172504</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.194780</td>\n      <td>1.141861</td>\n      <td>-0.860718</td>\n      <td>-1.013396</td>\n      <td>-1.069192</td>\n      <td>-0.552857</td>\n      <td>-0.181194</td>\n      <td>1.020571</td>\n      <td>-1.213755</td>\n      <td>-0.296157</td>\n      <td>...</td>\n      <td>-0.063017</td>\n      <td>-0.148142</td>\n      <td>-1.784751</td>\n      <td>1.201975</td>\n      <td>-1.022463</td>\n      <td>0.221809</td>\n      <td>-0.767619</td>\n      <td>0.941025</td>\n      <td>-1.583400</td>\n      <td>2.624626</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>112450</th>\n      <td>-0.133160</td>\n      <td>-0.669898</td>\n      <td>0.272018</td>\n      <td>0.602705</td>\n      <td>0.497571</td>\n      <td>-0.091507</td>\n      <td>0.424125</td>\n      <td>-0.346555</td>\n      <td>0.084319</td>\n      <td>-1.099335</td>\n      <td>...</td>\n      <td>-0.470875</td>\n      <td>-0.623940</td>\n      <td>-0.557836</td>\n      <td>-0.275097</td>\n      <td>-0.082015</td>\n      <td>-0.352531</td>\n      <td>0.565829</td>\n      <td>-0.353458</td>\n      <td>-0.051545</td>\n      <td>0.009822</td>\n    </tr>\n    <tr>\n      <th>112451</th>\n      <td>-0.262421</td>\n      <td>-0.089078</td>\n      <td>0.276614</td>\n      <td>-0.028210</td>\n      <td>0.196182</td>\n      <td>0.112838</td>\n      <td>1.198047</td>\n      <td>-0.642695</td>\n      <td>-0.807168</td>\n      <td>0.112235</td>\n      <td>...</td>\n      <td>0.262296</td>\n      <td>0.962201</td>\n      <td>-1.422439</td>\n      <td>-1.717809</td>\n      <td>0.246690</td>\n      <td>-0.858461</td>\n      <td>0.601352</td>\n      <td>0.470246</td>\n      <td>0.015822</td>\n      <td>0.495168</td>\n    </tr>\n    <tr>\n      <th>112452</th>\n      <td>2.366706</td>\n      <td>-1.512846</td>\n      <td>1.166286</td>\n      <td>0.784500</td>\n      <td>0.992960</td>\n      <td>1.409006</td>\n      <td>-0.042223</td>\n      <td>-0.704344</td>\n      <td>-0.166065</td>\n      <td>-0.228819</td>\n      <td>...</td>\n      <td>1.524984</td>\n      <td>0.901985</td>\n      <td>0.707583</td>\n      <td>0.547474</td>\n      <td>0.926508</td>\n      <td>0.417688</td>\n      <td>1.032225</td>\n      <td>1.036761</td>\n      <td>0.784477</td>\n      <td>0.441887</td>\n    </tr>\n    <tr>\n      <th>112453</th>\n      <td>-0.996360</td>\n      <td>-0.528871</td>\n      <td>0.698901</td>\n      <td>0.757775</td>\n      <td>0.812551</td>\n      <td>0.348989</td>\n      <td>0.063959</td>\n      <td>-0.708739</td>\n      <td>1.677042</td>\n      <td>0.055551</td>\n      <td>...</td>\n      <td>-0.501684</td>\n      <td>-0.195061</td>\n      <td>0.241821</td>\n      <td>-0.083057</td>\n      <td>-0.283776</td>\n      <td>0.640558</td>\n      <td>-0.922673</td>\n      <td>-0.655744</td>\n      <td>-0.824914</td>\n      <td>-1.049133</td>\n    </tr>\n    <tr>\n      <th>112454</th>\n      <td>-0.599571</td>\n      <td>-0.446375</td>\n      <td>-0.200835</td>\n      <td>0.447156</td>\n      <td>0.057043</td>\n      <td>-0.589107</td>\n      <td>0.224776</td>\n      <td>0.027139</td>\n      <td>0.389206</td>\n      <td>0.462315</td>\n      <td>...</td>\n      <td>-0.072461</td>\n      <td>-0.636520</td>\n      <td>0.757840</td>\n      <td>0.872088</td>\n      <td>1.752921</td>\n      <td>1.078445</td>\n      <td>0.578571</td>\n      <td>-0.126062</td>\n      <td>-0.437330</td>\n      <td>1.209496</td>\n    </tr>\n  </tbody>\n</table>\n<p>112455 rows × 26 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\nnp.random.seed(42)\n\n# Base models\nrf = RandomForestClassifier(\n    n_estimators=200,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    max_features='log2',\n    max_depth=None,\n    bootstrap=False,\n    random_state=42\n)\nknn = KNeighborsClassifier(\n    n_neighbors=1)\n\ndt = DecisionTreeClassifier(\n\trandom_state=42)\n\n# Final estimator\nfinal_estimator = LogisticRegression(multi_class='multinomial', max_iter=2000)\n\n# Function to get out-of-fold predictions\ndef get_oof_pred(model, X, y, cv=3):\n    oof_pred = cross_val_predict(model, X, y, cv=cv, method='predict_proba')\n    return oof_pred\n\n# Get out-of-fold predictions for base models\nrf_oof = get_oof_pred(rf, X_train_normalized, y_train)\nknn_oof = get_oof_pred(knn, X_train_normalized, y_train)\ndt_oof = get_oof_pred(dt, X_train_normalized, y_train)\n\n# Combine out-of-fold predictions\nX_train_meta = np.hstack([rf_oof, knn_oof, dt_oof])\n\n# Fit final estimator\nfinal_estimator.fit(X_train_meta, y_train)\n\n# Fit base models on entire training data\nrf.fit(X_train_normalized, y_train)\nknn.fit(X_train_normalized, y_train)\ndt.fit(X_train_normalized, y_train)\n\n# Get predictions for test data\nrf_test = rf.predict_proba(X_test_normalized)\nknn_test = knn.predict_proba(X_test_normalized)\ndt_test = dt.predict_proba(X_test_normalized)\n\n# Combine test predictions\nX_test_meta = np.hstack([rf_test, knn_test, dt_test])\n\n# Final prediction\nfinal_pred = final_estimator.predict(X_test_meta)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, final_pred)\nprint(f\"Accuracy of the stacking model: {accuracy:.5f}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-06T10:35:34.448163Z","iopub.execute_input":"2024-07-06T10:35:34.448602Z","iopub.status.idle":"2024-07-06T10:58:34.518390Z","shell.execute_reply.started":"2024-07-06T10:35:34.448564Z","shell.execute_reply":"2024-07-06T10:58:34.515012Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Accuracy of the stacking model: 0.84783\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report\n\n# Generate and print classification report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, final_pred))","metadata":{"execution":{"iopub.status.busy":"2024-07-06T10:58:34.523271Z","iopub.execute_input":"2024-07-06T10:58:34.524095Z","iopub.status.idle":"2024-07-06T10:58:34.717040Z","shell.execute_reply.started":"2024-07-06T10:58:34.524049Z","shell.execute_reply":"2024-07-06T10:58:34.715738Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.85      0.77      0.81      1071\n           1       0.80      0.84      0.82      1071\n           2       0.88      0.81      0.84      1071\n           3       0.83      0.83      0.83      1071\n           4       0.85      0.85      0.85      1071\n           5       0.83      0.82      0.82      1071\n           6       0.83      0.65      0.73      1071\n           7       0.85      0.93      0.89      1071\n           8       0.87      0.84      0.85      1071\n           9       0.89      0.92      0.90      1071\n          10       0.82      0.77      0.79      1071\n          11       0.82      0.82      0.82      1071\n          12       0.80      0.82      0.81      1071\n          13       0.88      0.92      0.90      1071\n          14       0.85      0.83      0.84      1071\n          15       0.85      0.90      0.88      1071\n          16       0.92      0.96      0.94      1071\n          17       0.85      0.84      0.85      1071\n          18       0.86      0.89      0.88      1071\n          19       0.87      0.85      0.86      1071\n          20       0.91      0.87      0.89      1071\n          21       0.86      0.90      0.88      1071\n          22       0.80      0.85      0.82      1071\n          23       0.89      0.89      0.89      1071\n          24       0.83      0.71      0.76      1071\n          25       0.84      0.88      0.86      1071\n          26       0.85      0.88      0.86      1071\n          27       0.89      0.84      0.87      1071\n          28       0.87      0.90      0.89      1071\n          29       0.82      0.88      0.85      1071\n          30       0.81      0.74      0.77      1071\n          31       0.80      0.77      0.79      1071\n          32       0.79      0.79      0.79      1071\n          33       0.85      0.92      0.88      1071\n          34       0.84      0.93      0.88      1071\n          35       0.83      0.84      0.83      1071\n          36       0.83      0.95      0.89      1071\n          37       0.94      0.95      0.94      1071\n          38       0.83      0.83      0.83      1071\n          39       0.85      0.80      0.82      1071\n          40       0.85      0.89      0.87      1071\n          41       0.87      0.83      0.85      1071\n          42       0.84      0.79      0.81      1071\n          43       0.81      0.85      0.83      1071\n          44       0.84      0.84      0.84      1071\n\n    accuracy                           0.85     48195\n   macro avg       0.85      0.85      0.85     48195\nweighted avg       0.85      0.85      0.85     48195\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}