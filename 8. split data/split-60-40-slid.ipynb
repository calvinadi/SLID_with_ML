{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8848655,"sourceType":"datasetVersion","datasetId":5326116}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-06T11:07:14.781053Z","iopub.execute_input":"2024-07-06T11:07:14.781476Z","iopub.status.idle":"2024-07-06T11:07:15.649685Z","shell.execute_reply.started":"2024-07-06T11:07:14.781439Z","shell.execute_reply":"2024-07-06T11:07:15.648414Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nimport joblib\nfrom sklearn.model_selection import KFold","metadata":{"execution":{"iopub.status.busy":"2024-07-06T11:07:15.651944Z","iopub.execute_input":"2024-07-06T11:07:15.652590Z","iopub.status.idle":"2024-07-06T11:07:15.658753Z","shell.execute_reply.started":"2024-07-06T11:07:15.652540Z","shell.execute_reply":"2024-07-06T11:07:15.657515Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(r\"/kaggle/input/feature-common-language/audio_features_partial.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T11:07:15.660548Z","iopub.execute_input":"2024-07-06T11:07:15.660980Z","iopub.status.idle":"2024-07-06T11:07:16.537446Z","shell.execute_reply.started":"2024-07-06T11:07:15.660919Z","shell.execute_reply":"2024-07-06T11:07:16.536341Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   chroma_stft       rms  spectral_centroid  spectral_bandwidth  \\\n0     0.712224  0.055740        2989.050985         2193.800068   \n1     0.598403  0.074004        2372.315827         2065.561020   \n2     0.756316  0.046051        3274.178654         2196.474265   \n3     0.588983  0.061376        1948.418292         2049.242741   \n4     0.647222  0.069638        1705.618989         1824.714129   \n\n   spectral_rolloff  zero_crossing_rate     mfcc_1     mfcc_2     mfcc_3  \\\n0       5666.294643            0.275144 -202.32579  27.614292   4.094784   \n1       4795.649510            0.178041 -137.41476  59.931843   5.077963   \n2       5870.572917            0.353054 -233.02031  22.691550  10.057923   \n3       4186.921296            0.137682 -199.04490  80.806870  31.451380   \n4       3730.709877            0.095540 -325.47556  92.373820  17.725632   \n\n      mfcc_4  ...    mfcc_13   mfcc_14    mfcc_15   mfcc_16   mfcc_17  \\\n0   5.301181  ...   4.752774 -0.985637  -6.752584 -4.679379 -5.478848   \n1  -5.712012  ... -12.469353 -2.134825 -10.989368 -1.460541 -4.485021   \n2   3.829097  ...  -1.657243 -5.409642  -4.017134 -6.744406 -1.697630   \n3  -1.297673  ...  -4.202263  0.065943  -9.312079 -7.164060 -3.082040   \n4  31.867613  ...   3.169903  4.538502  -1.073114 -1.204524 -0.108214   \n\n    mfcc_18   mfcc_19   mfcc_20   label  \\\n0 -0.866508 -1.919669 -0.634521  Arabic   \n1 -0.408789 -8.211143 -5.170048  Arabic   \n2 -0.387302  0.829549  1.292110  Arabic   \n3 -8.046175 -3.083879 -2.018449  Arabic   \n4 -4.803460 -2.882802 -1.455632  Arabic   \n\n                                           file_path  \n0  /kaggle/input/preprocess-common-language/proce...  \n1  /kaggle/input/preprocess-common-language/proce...  \n2  /kaggle/input/preprocess-common-language/proce...  \n3  /kaggle/input/preprocess-common-language/proce...  \n4  /kaggle/input/preprocess-common-language/proce...  \n\n[5 rows x 28 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chroma_stft</th>\n      <th>rms</th>\n      <th>spectral_centroid</th>\n      <th>spectral_bandwidth</th>\n      <th>spectral_rolloff</th>\n      <th>zero_crossing_rate</th>\n      <th>mfcc_1</th>\n      <th>mfcc_2</th>\n      <th>mfcc_3</th>\n      <th>mfcc_4</th>\n      <th>...</th>\n      <th>mfcc_13</th>\n      <th>mfcc_14</th>\n      <th>mfcc_15</th>\n      <th>mfcc_16</th>\n      <th>mfcc_17</th>\n      <th>mfcc_18</th>\n      <th>mfcc_19</th>\n      <th>mfcc_20</th>\n      <th>label</th>\n      <th>file_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.712224</td>\n      <td>0.055740</td>\n      <td>2989.050985</td>\n      <td>2193.800068</td>\n      <td>5666.294643</td>\n      <td>0.275144</td>\n      <td>-202.32579</td>\n      <td>27.614292</td>\n      <td>4.094784</td>\n      <td>5.301181</td>\n      <td>...</td>\n      <td>4.752774</td>\n      <td>-0.985637</td>\n      <td>-6.752584</td>\n      <td>-4.679379</td>\n      <td>-5.478848</td>\n      <td>-0.866508</td>\n      <td>-1.919669</td>\n      <td>-0.634521</td>\n      <td>Arabic</td>\n      <td>/kaggle/input/preprocess-common-language/proce...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.598403</td>\n      <td>0.074004</td>\n      <td>2372.315827</td>\n      <td>2065.561020</td>\n      <td>4795.649510</td>\n      <td>0.178041</td>\n      <td>-137.41476</td>\n      <td>59.931843</td>\n      <td>5.077963</td>\n      <td>-5.712012</td>\n      <td>...</td>\n      <td>-12.469353</td>\n      <td>-2.134825</td>\n      <td>-10.989368</td>\n      <td>-1.460541</td>\n      <td>-4.485021</td>\n      <td>-0.408789</td>\n      <td>-8.211143</td>\n      <td>-5.170048</td>\n      <td>Arabic</td>\n      <td>/kaggle/input/preprocess-common-language/proce...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.756316</td>\n      <td>0.046051</td>\n      <td>3274.178654</td>\n      <td>2196.474265</td>\n      <td>5870.572917</td>\n      <td>0.353054</td>\n      <td>-233.02031</td>\n      <td>22.691550</td>\n      <td>10.057923</td>\n      <td>3.829097</td>\n      <td>...</td>\n      <td>-1.657243</td>\n      <td>-5.409642</td>\n      <td>-4.017134</td>\n      <td>-6.744406</td>\n      <td>-1.697630</td>\n      <td>-0.387302</td>\n      <td>0.829549</td>\n      <td>1.292110</td>\n      <td>Arabic</td>\n      <td>/kaggle/input/preprocess-common-language/proce...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.588983</td>\n      <td>0.061376</td>\n      <td>1948.418292</td>\n      <td>2049.242741</td>\n      <td>4186.921296</td>\n      <td>0.137682</td>\n      <td>-199.04490</td>\n      <td>80.806870</td>\n      <td>31.451380</td>\n      <td>-1.297673</td>\n      <td>...</td>\n      <td>-4.202263</td>\n      <td>0.065943</td>\n      <td>-9.312079</td>\n      <td>-7.164060</td>\n      <td>-3.082040</td>\n      <td>-8.046175</td>\n      <td>-3.083879</td>\n      <td>-2.018449</td>\n      <td>Arabic</td>\n      <td>/kaggle/input/preprocess-common-language/proce...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.647222</td>\n      <td>0.069638</td>\n      <td>1705.618989</td>\n      <td>1824.714129</td>\n      <td>3730.709877</td>\n      <td>0.095540</td>\n      <td>-325.47556</td>\n      <td>92.373820</td>\n      <td>17.725632</td>\n      <td>31.867613</td>\n      <td>...</td>\n      <td>3.169903</td>\n      <td>4.538502</td>\n      <td>-1.073114</td>\n      <td>-1.204524</td>\n      <td>-0.108214</td>\n      <td>-4.803460</td>\n      <td>-2.882802</td>\n      <td>-1.455632</td>\n      <td>Arabic</td>\n      <td>/kaggle/input/preprocess-common-language/proce...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 28 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T11:07:16.540658Z","iopub.execute_input":"2024-07-06T11:07:16.541124Z","iopub.status.idle":"2024-07-06T11:07:16.775233Z","shell.execute_reply.started":"2024-07-06T11:07:16.541076Z","shell.execute_reply":"2024-07-06T11:07:16.774169Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Initialize LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Fit LabelEncoder with your actual labels\nlabel_encoder.fit(df['label'])\n\n# Transform actual labels to numeric labels\ndf['numeric_labels'] = label_encoder.transform(df['label'])","metadata":{"execution":{"iopub.status.busy":"2024-07-06T11:07:16.776725Z","iopub.execute_input":"2024-07-06T11:07:16.777156Z","iopub.status.idle":"2024-07-06T11:07:16.804769Z","shell.execute_reply.started":"2024-07-06T11:07:16.777118Z","shell.execute_reply":"2024-07-06T11:07:16.803461Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"X = df.drop(columns=['label','numeric_labels','file_path'])\ny = df['numeric_labels']","metadata":{"execution":{"iopub.status.busy":"2024-07-06T11:07:16.806138Z","iopub.execute_input":"2024-07-06T11:07:16.806478Z","iopub.status.idle":"2024-07-06T11:07:16.825446Z","shell.execute_reply.started":"2024-07-06T11:07:16.806450Z","shell.execute_reply":"2024-07-06T11:07:16.824051Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(sampling_strategy='auto', random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X, y)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T11:07:16.826688Z","iopub.execute_input":"2024-07-06T11:07:16.827012Z","iopub.status.idle":"2024-07-06T11:07:18.035561Z","shell.execute_reply.started":"2024-07-06T11:07:16.826983Z","shell.execute_reply":"2024-07-06T11:07:18.034469Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\n\n# Jumlah kelas sebelum SMOTE\nprint(\"Jumlah kelas sebelum SMOTE:\")\nprint(Counter(y))\n\n# Jumlah kelas setelah SMOTE\nprint(\"\\nJumlah kelas setelah SMOTE:\")\nprint(Counter(y_resampled))","metadata":{"execution":{"iopub.status.busy":"2024-07-06T11:07:18.036852Z","iopub.execute_input":"2024-07-06T11:07:18.037309Z","iopub.status.idle":"2024-07-06T11:07:18.079883Z","shell.execute_reply.started":"2024-07-06T11:07:18.037278Z","shell.execute_reply":"2024-07-06T11:07:18.078410Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Jumlah kelas sebelum SMOTE:\nCounter({2: 3570, 39: 3177, 19: 3012, 6: 3003, 27: 2955, 41: 2784, 24: 2766, 0: 2751, 20: 2733, 37: 2691, 10: 2550, 33: 2547, 18: 2520, 21: 2484, 42: 2472, 8: 2442, 40: 2367, 26: 2331, 30: 2283, 28: 2277, 23: 2268, 32: 2238, 7: 2202, 31: 2196, 43: 2181, 15: 2073, 44: 2073, 5: 2016, 34: 1977, 9: 1962, 25: 1947, 12: 1932, 1: 1914, 16: 1890, 29: 1884, 14: 1863, 3: 1830, 17: 1830, 4: 1797, 11: 1773, 35: 1758, 36: 1752, 38: 1737, 22: 1695, 13: 1623})\n\nJumlah kelas setelah SMOTE:\nCounter({0: 3570, 4: 3570, 9: 3570, 27: 3570, 18: 3570, 24: 3570, 23: 3570, 13: 3570, 20: 3570, 11: 3570, 16: 3570, 40: 3570, 28: 3570, 15: 3570, 21: 3570, 8: 3570, 31: 3570, 29: 3570, 5: 3570, 7: 3570, 41: 3570, 44: 3570, 3: 3570, 2: 3570, 14: 3570, 43: 3570, 36: 3570, 26: 3570, 34: 3570, 12: 3570, 37: 3570, 35: 3570, 42: 3570, 33: 3570, 17: 3570, 25: 3570, 6: 3570, 30: 3570, 39: 3570, 22: 3570, 10: 3570, 1: 3570, 19: 3570, 32: 3570, 38: 3570})\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled,\n                                                    test_size=0.4, random_state=42,\n                                                    stratify=y_resampled)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T11:07:18.081798Z","iopub.execute_input":"2024-07-06T11:07:18.082231Z","iopub.status.idle":"2024-07-06T11:07:18.220739Z","shell.execute_reply.started":"2024-07-06T11:07:18.082194Z","shell.execute_reply":"2024-07-06T11:07:18.219424Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Inisialisasi StandardScaler\nscaler = StandardScaler()\n\n# Fit scaler pada data training dan transform kedua set data\nX_train_normalized = scaler.fit_transform(X_train)\nX_test_normalized = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T11:07:18.223517Z","iopub.execute_input":"2024-07-06T11:07:18.223869Z","iopub.status.idle":"2024-07-06T11:07:18.295452Z","shell.execute_reply.started":"2024-07-06T11:07:18.223839Z","shell.execute_reply":"2024-07-06T11:07:18.293988Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(X_train_normalized)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T11:07:18.296804Z","iopub.execute_input":"2024-07-06T11:07:18.297165Z","iopub.status.idle":"2024-07-06T11:07:18.332208Z","shell.execute_reply.started":"2024-07-06T11:07:18.297134Z","shell.execute_reply":"2024-07-06T11:07:18.330968Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"             0         1         2         3         4         5         6   \\\n0      0.145148 -0.243775 -0.564325  0.295038 -0.016204 -1.142099  1.153783   \n1      0.363284 -0.534977  1.895347  0.310785  1.191994  2.052806  1.419636   \n2      0.286403  2.070311 -2.235751 -2.774938 -2.399756 -1.486115 -0.108897   \n3      1.540735 -0.878150  1.839851  0.895095  1.471534  2.211712 -0.384746   \n4      0.103125 -0.092777 -0.462252  0.276458 -0.123571 -0.637858  0.847346   \n...         ...       ...       ...       ...       ...       ...       ...   \n96385 -0.555811  0.933177 -1.697864 -2.425019 -1.826446 -1.024454 -2.628052   \n96386 -1.164534  0.435379 -0.297543  0.597439  0.155819 -0.859657  0.617702   \n96387  1.807935 -0.634323  1.817252  1.269317  1.780937  1.445186  2.010528   \n96388 -0.284003  0.978399 -1.620108 -1.732164 -1.776316 -1.207074 -1.860547   \n96389  0.422919 -0.145748 -0.180167  0.331022  0.028811 -0.142096  1.122150   \n\n             7         8         9   ...        16        17        18  \\\n0      0.380361 -0.185337 -0.710902  ...  0.028347 -1.056303 -0.617371   \n1     -1.999787 -0.410735 -0.693735  ...  0.133048 -0.504716 -0.847350   \n2      3.522644 -2.830103  0.723230  ... -1.488588  0.494795 -2.456101   \n3     -1.535593 -0.497176 -0.009190  ...  0.589678  0.562013  0.796310   \n4      0.213283  1.228329  0.608387  ... -0.974766 -1.162272  0.085872   \n...         ...       ...       ...  ...       ...       ...       ...   \n96385  1.703265 -0.264701  1.900168  ... -0.162538 -0.791527 -0.874288   \n96386  0.225779  0.645715 -0.555870  ...  0.174678  0.628114  0.854186   \n96387 -1.370506  0.180721 -0.062834  ...  1.496967  1.190808  1.084793   \n96388  1.342178 -1.274138 -0.221893  ... -1.198759  1.045167 -2.460497   \n96389  0.308598  0.766608 -0.304685  ...  1.811203  0.712509 -1.461870   \n\n             19        20        21        22        23        24        25  \n0     -0.303022  0.108203 -0.631353 -0.774307 -0.541132 -0.867935 -0.430604  \n1      0.304878  0.172084  1.168591  1.652994  1.508829  0.820410  0.721710  \n2      1.526978 -0.016101 -2.648112  1.190158  0.590908 -1.678454  1.657256  \n3      0.491143  0.431363  0.810643  0.535725 -0.580483  1.316392  0.058985  \n4      0.292758 -1.123345  0.089794  0.369486 -0.270559 -0.173030  0.172181  \n...         ...       ...       ...       ...       ...       ...       ...  \n96385 -0.329407 -0.167591  0.037036 -0.127334 -0.622895 -3.104520 -2.590298  \n96386 -0.498109 -0.053831 -0.280880 -0.730954 -0.913117 -0.030662 -0.009050  \n96387  1.460355  1.147283  1.320554  0.703466  1.554040  1.878891  1.395960  \n96388 -0.413047 -0.319548 -2.127616  0.538789  1.513587 -1.830264  0.579055  \n96389  0.141199 -0.476710  0.191346  0.468733  0.212089 -0.512781  0.193409  \n\n[96390 rows x 26 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.145148</td>\n      <td>-0.243775</td>\n      <td>-0.564325</td>\n      <td>0.295038</td>\n      <td>-0.016204</td>\n      <td>-1.142099</td>\n      <td>1.153783</td>\n      <td>0.380361</td>\n      <td>-0.185337</td>\n      <td>-0.710902</td>\n      <td>...</td>\n      <td>0.028347</td>\n      <td>-1.056303</td>\n      <td>-0.617371</td>\n      <td>-0.303022</td>\n      <td>0.108203</td>\n      <td>-0.631353</td>\n      <td>-0.774307</td>\n      <td>-0.541132</td>\n      <td>-0.867935</td>\n      <td>-0.430604</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.363284</td>\n      <td>-0.534977</td>\n      <td>1.895347</td>\n      <td>0.310785</td>\n      <td>1.191994</td>\n      <td>2.052806</td>\n      <td>1.419636</td>\n      <td>-1.999787</td>\n      <td>-0.410735</td>\n      <td>-0.693735</td>\n      <td>...</td>\n      <td>0.133048</td>\n      <td>-0.504716</td>\n      <td>-0.847350</td>\n      <td>0.304878</td>\n      <td>0.172084</td>\n      <td>1.168591</td>\n      <td>1.652994</td>\n      <td>1.508829</td>\n      <td>0.820410</td>\n      <td>0.721710</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.286403</td>\n      <td>2.070311</td>\n      <td>-2.235751</td>\n      <td>-2.774938</td>\n      <td>-2.399756</td>\n      <td>-1.486115</td>\n      <td>-0.108897</td>\n      <td>3.522644</td>\n      <td>-2.830103</td>\n      <td>0.723230</td>\n      <td>...</td>\n      <td>-1.488588</td>\n      <td>0.494795</td>\n      <td>-2.456101</td>\n      <td>1.526978</td>\n      <td>-0.016101</td>\n      <td>-2.648112</td>\n      <td>1.190158</td>\n      <td>0.590908</td>\n      <td>-1.678454</td>\n      <td>1.657256</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.540735</td>\n      <td>-0.878150</td>\n      <td>1.839851</td>\n      <td>0.895095</td>\n      <td>1.471534</td>\n      <td>2.211712</td>\n      <td>-0.384746</td>\n      <td>-1.535593</td>\n      <td>-0.497176</td>\n      <td>-0.009190</td>\n      <td>...</td>\n      <td>0.589678</td>\n      <td>0.562013</td>\n      <td>0.796310</td>\n      <td>0.491143</td>\n      <td>0.431363</td>\n      <td>0.810643</td>\n      <td>0.535725</td>\n      <td>-0.580483</td>\n      <td>1.316392</td>\n      <td>0.058985</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.103125</td>\n      <td>-0.092777</td>\n      <td>-0.462252</td>\n      <td>0.276458</td>\n      <td>-0.123571</td>\n      <td>-0.637858</td>\n      <td>0.847346</td>\n      <td>0.213283</td>\n      <td>1.228329</td>\n      <td>0.608387</td>\n      <td>...</td>\n      <td>-0.974766</td>\n      <td>-1.162272</td>\n      <td>0.085872</td>\n      <td>0.292758</td>\n      <td>-1.123345</td>\n      <td>0.089794</td>\n      <td>0.369486</td>\n      <td>-0.270559</td>\n      <td>-0.173030</td>\n      <td>0.172181</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>96385</th>\n      <td>-0.555811</td>\n      <td>0.933177</td>\n      <td>-1.697864</td>\n      <td>-2.425019</td>\n      <td>-1.826446</td>\n      <td>-1.024454</td>\n      <td>-2.628052</td>\n      <td>1.703265</td>\n      <td>-0.264701</td>\n      <td>1.900168</td>\n      <td>...</td>\n      <td>-0.162538</td>\n      <td>-0.791527</td>\n      <td>-0.874288</td>\n      <td>-0.329407</td>\n      <td>-0.167591</td>\n      <td>0.037036</td>\n      <td>-0.127334</td>\n      <td>-0.622895</td>\n      <td>-3.104520</td>\n      <td>-2.590298</td>\n    </tr>\n    <tr>\n      <th>96386</th>\n      <td>-1.164534</td>\n      <td>0.435379</td>\n      <td>-0.297543</td>\n      <td>0.597439</td>\n      <td>0.155819</td>\n      <td>-0.859657</td>\n      <td>0.617702</td>\n      <td>0.225779</td>\n      <td>0.645715</td>\n      <td>-0.555870</td>\n      <td>...</td>\n      <td>0.174678</td>\n      <td>0.628114</td>\n      <td>0.854186</td>\n      <td>-0.498109</td>\n      <td>-0.053831</td>\n      <td>-0.280880</td>\n      <td>-0.730954</td>\n      <td>-0.913117</td>\n      <td>-0.030662</td>\n      <td>-0.009050</td>\n    </tr>\n    <tr>\n      <th>96387</th>\n      <td>1.807935</td>\n      <td>-0.634323</td>\n      <td>1.817252</td>\n      <td>1.269317</td>\n      <td>1.780937</td>\n      <td>1.445186</td>\n      <td>2.010528</td>\n      <td>-1.370506</td>\n      <td>0.180721</td>\n      <td>-0.062834</td>\n      <td>...</td>\n      <td>1.496967</td>\n      <td>1.190808</td>\n      <td>1.084793</td>\n      <td>1.460355</td>\n      <td>1.147283</td>\n      <td>1.320554</td>\n      <td>0.703466</td>\n      <td>1.554040</td>\n      <td>1.878891</td>\n      <td>1.395960</td>\n    </tr>\n    <tr>\n      <th>96388</th>\n      <td>-0.284003</td>\n      <td>0.978399</td>\n      <td>-1.620108</td>\n      <td>-1.732164</td>\n      <td>-1.776316</td>\n      <td>-1.207074</td>\n      <td>-1.860547</td>\n      <td>1.342178</td>\n      <td>-1.274138</td>\n      <td>-0.221893</td>\n      <td>...</td>\n      <td>-1.198759</td>\n      <td>1.045167</td>\n      <td>-2.460497</td>\n      <td>-0.413047</td>\n      <td>-0.319548</td>\n      <td>-2.127616</td>\n      <td>0.538789</td>\n      <td>1.513587</td>\n      <td>-1.830264</td>\n      <td>0.579055</td>\n    </tr>\n    <tr>\n      <th>96389</th>\n      <td>0.422919</td>\n      <td>-0.145748</td>\n      <td>-0.180167</td>\n      <td>0.331022</td>\n      <td>0.028811</td>\n      <td>-0.142096</td>\n      <td>1.122150</td>\n      <td>0.308598</td>\n      <td>0.766608</td>\n      <td>-0.304685</td>\n      <td>...</td>\n      <td>1.811203</td>\n      <td>0.712509</td>\n      <td>-1.461870</td>\n      <td>0.141199</td>\n      <td>-0.476710</td>\n      <td>0.191346</td>\n      <td>0.468733</td>\n      <td>0.212089</td>\n      <td>-0.512781</td>\n      <td>0.193409</td>\n    </tr>\n  </tbody>\n</table>\n<p>96390 rows × 26 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\nnp.random.seed(42)\n\n# Base models\nrf = RandomForestClassifier(\n    n_estimators=200,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    max_features='log2',\n    max_depth=None,\n    bootstrap=False,\n    random_state=42\n)\nknn = KNeighborsClassifier(\n    n_neighbors=1)\n\ndt = DecisionTreeClassifier(\n\trandom_state=42)\n\n# Final estimator\nfinal_estimator = LogisticRegression(multi_class='multinomial', max_iter=2000)\n\n# Function to get out-of-fold predictions\ndef get_oof_pred(model, X, y, cv=3):\n    oof_pred = cross_val_predict(model, X, y, cv=cv, method='predict_proba')\n    return oof_pred\n\n# Get out-of-fold predictions for base models\nrf_oof = get_oof_pred(rf, X_train_normalized, y_train)\nknn_oof = get_oof_pred(knn, X_train_normalized, y_train)\ndt_oof = get_oof_pred(dt, X_train_normalized, y_train)\n\n# Combine out-of-fold predictions\nX_train_meta = np.hstack([rf_oof, knn_oof, dt_oof])\n\n# Fit final estimator\nfinal_estimator.fit(X_train_meta, y_train)\n\n# Fit base models on entire training data\nrf.fit(X_train_normalized, y_train)\nknn.fit(X_train_normalized, y_train)\ndt.fit(X_train_normalized, y_train)\n\n# Get predictions for test data\nrf_test = rf.predict_proba(X_test_normalized)\nknn_test = knn.predict_proba(X_test_normalized)\ndt_test = dt.predict_proba(X_test_normalized)\n\n# Combine test predictions\nX_test_meta = np.hstack([rf_test, knn_test, dt_test])\n\n# Final prediction\nfinal_pred = final_estimator.predict(X_test_meta)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, final_pred)\nprint(f\"Accuracy of the stacking model: {accuracy:.5f}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-06T11:07:18.333692Z","iopub.execute_input":"2024-07-06T11:07:18.334143Z","iopub.status.idle":"2024-07-06T11:26:54.344297Z","shell.execute_reply.started":"2024-07-06T11:07:18.334103Z","shell.execute_reply":"2024-07-06T11:26:54.342703Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Accuracy of the stacking model: 0.81418\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report\n\n# Generate and print classification report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, final_pred))","metadata":{"execution":{"iopub.status.busy":"2024-07-06T11:26:54.345982Z","iopub.execute_input":"2024-07-06T11:26:54.347122Z","iopub.status.idle":"2024-07-06T11:26:54.590249Z","shell.execute_reply.started":"2024-07-06T11:26:54.347070Z","shell.execute_reply":"2024-07-06T11:26:54.589015Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.81      0.73      0.77      1428\n           1       0.75      0.81      0.78      1428\n           2       0.86      0.79      0.82      1428\n           3       0.78      0.81      0.79      1428\n           4       0.82      0.81      0.81      1428\n           5       0.77      0.78      0.78      1428\n           6       0.78      0.59      0.67      1428\n           7       0.82      0.92      0.87      1428\n           8       0.83      0.80      0.81      1428\n           9       0.85      0.90      0.88      1428\n          10       0.80      0.73      0.76      1428\n          11       0.78      0.78      0.78      1428\n          12       0.75      0.78      0.77      1428\n          13       0.85      0.88      0.86      1428\n          14       0.81      0.78      0.80      1428\n          15       0.82      0.87      0.84      1428\n          16       0.90      0.95      0.92      1428\n          17       0.80      0.80      0.80      1428\n          18       0.85      0.87      0.86      1428\n          19       0.84      0.82      0.83      1428\n          20       0.89      0.86      0.88      1428\n          21       0.85      0.88      0.87      1428\n          22       0.76      0.81      0.78      1428\n          23       0.85      0.87      0.86      1428\n          24       0.79      0.65      0.71      1428\n          25       0.81      0.86      0.83      1428\n          26       0.81      0.85      0.83      1428\n          27       0.85      0.81      0.83      1428\n          28       0.86      0.88      0.87      1428\n          29       0.79      0.84      0.81      1428\n          30       0.77      0.68      0.72      1428\n          31       0.77      0.70      0.73      1428\n          32       0.74      0.75      0.74      1428\n          33       0.82      0.89      0.85      1428\n          34       0.82      0.91      0.86      1428\n          35       0.81      0.81      0.81      1428\n          36       0.82      0.93      0.87      1428\n          37       0.93      0.93      0.93      1428\n          38       0.81      0.78      0.80      1428\n          39       0.80      0.75      0.78      1428\n          40       0.81      0.87      0.84      1428\n          41       0.82      0.79      0.81      1428\n          42       0.80      0.73      0.76      1428\n          43       0.77      0.81      0.79      1428\n          44       0.80      0.78      0.79      1428\n\n    accuracy                           0.81     64260\n   macro avg       0.81      0.81      0.81     64260\nweighted avg       0.81      0.81      0.81     64260\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}