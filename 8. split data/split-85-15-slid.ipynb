{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8848655,"sourceType":"datasetVersion","datasetId":5326116}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-06T11:45:43.769775Z","iopub.execute_input":"2024-07-06T11:45:43.770189Z","iopub.status.idle":"2024-07-06T11:45:44.686171Z","shell.execute_reply.started":"2024-07-06T11:45:43.770149Z","shell.execute_reply":"2024-07-06T11:45:44.685155Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nimport joblib\nfrom sklearn.model_selection import KFold","metadata":{"execution":{"iopub.status.busy":"2024-07-06T11:45:44.692097Z","iopub.execute_input":"2024-07-06T11:45:44.692445Z","iopub.status.idle":"2024-07-06T11:45:44.697978Z","shell.execute_reply.started":"2024-07-06T11:45:44.692417Z","shell.execute_reply":"2024-07-06T11:45:44.696731Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(r\"/kaggle/input/feature-common-language/audio_features_partial.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T11:45:44.699449Z","iopub.execute_input":"2024-07-06T11:45:44.699879Z","iopub.status.idle":"2024-07-06T11:45:45.686505Z","shell.execute_reply.started":"2024-07-06T11:45:44.699842Z","shell.execute_reply":"2024-07-06T11:45:45.685370Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   chroma_stft       rms  spectral_centroid  spectral_bandwidth  \\\n0     0.712224  0.055740        2989.050985         2193.800068   \n1     0.598403  0.074004        2372.315827         2065.561020   \n2     0.756316  0.046051        3274.178654         2196.474265   \n3     0.588983  0.061376        1948.418292         2049.242741   \n4     0.647222  0.069638        1705.618989         1824.714129   \n\n   spectral_rolloff  zero_crossing_rate     mfcc_1     mfcc_2     mfcc_3  \\\n0       5666.294643            0.275144 -202.32579  27.614292   4.094784   \n1       4795.649510            0.178041 -137.41476  59.931843   5.077963   \n2       5870.572917            0.353054 -233.02031  22.691550  10.057923   \n3       4186.921296            0.137682 -199.04490  80.806870  31.451380   \n4       3730.709877            0.095540 -325.47556  92.373820  17.725632   \n\n      mfcc_4  ...    mfcc_13   mfcc_14    mfcc_15   mfcc_16   mfcc_17  \\\n0   5.301181  ...   4.752774 -0.985637  -6.752584 -4.679379 -5.478848   \n1  -5.712012  ... -12.469353 -2.134825 -10.989368 -1.460541 -4.485021   \n2   3.829097  ...  -1.657243 -5.409642  -4.017134 -6.744406 -1.697630   \n3  -1.297673  ...  -4.202263  0.065943  -9.312079 -7.164060 -3.082040   \n4  31.867613  ...   3.169903  4.538502  -1.073114 -1.204524 -0.108214   \n\n    mfcc_18   mfcc_19   mfcc_20   label  \\\n0 -0.866508 -1.919669 -0.634521  Arabic   \n1 -0.408789 -8.211143 -5.170048  Arabic   \n2 -0.387302  0.829549  1.292110  Arabic   \n3 -8.046175 -3.083879 -2.018449  Arabic   \n4 -4.803460 -2.882802 -1.455632  Arabic   \n\n                                           file_path  \n0  /kaggle/input/preprocess-common-language/proce...  \n1  /kaggle/input/preprocess-common-language/proce...  \n2  /kaggle/input/preprocess-common-language/proce...  \n3  /kaggle/input/preprocess-common-language/proce...  \n4  /kaggle/input/preprocess-common-language/proce...  \n\n[5 rows x 28 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chroma_stft</th>\n      <th>rms</th>\n      <th>spectral_centroid</th>\n      <th>spectral_bandwidth</th>\n      <th>spectral_rolloff</th>\n      <th>zero_crossing_rate</th>\n      <th>mfcc_1</th>\n      <th>mfcc_2</th>\n      <th>mfcc_3</th>\n      <th>mfcc_4</th>\n      <th>...</th>\n      <th>mfcc_13</th>\n      <th>mfcc_14</th>\n      <th>mfcc_15</th>\n      <th>mfcc_16</th>\n      <th>mfcc_17</th>\n      <th>mfcc_18</th>\n      <th>mfcc_19</th>\n      <th>mfcc_20</th>\n      <th>label</th>\n      <th>file_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.712224</td>\n      <td>0.055740</td>\n      <td>2989.050985</td>\n      <td>2193.800068</td>\n      <td>5666.294643</td>\n      <td>0.275144</td>\n      <td>-202.32579</td>\n      <td>27.614292</td>\n      <td>4.094784</td>\n      <td>5.301181</td>\n      <td>...</td>\n      <td>4.752774</td>\n      <td>-0.985637</td>\n      <td>-6.752584</td>\n      <td>-4.679379</td>\n      <td>-5.478848</td>\n      <td>-0.866508</td>\n      <td>-1.919669</td>\n      <td>-0.634521</td>\n      <td>Arabic</td>\n      <td>/kaggle/input/preprocess-common-language/proce...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.598403</td>\n      <td>0.074004</td>\n      <td>2372.315827</td>\n      <td>2065.561020</td>\n      <td>4795.649510</td>\n      <td>0.178041</td>\n      <td>-137.41476</td>\n      <td>59.931843</td>\n      <td>5.077963</td>\n      <td>-5.712012</td>\n      <td>...</td>\n      <td>-12.469353</td>\n      <td>-2.134825</td>\n      <td>-10.989368</td>\n      <td>-1.460541</td>\n      <td>-4.485021</td>\n      <td>-0.408789</td>\n      <td>-8.211143</td>\n      <td>-5.170048</td>\n      <td>Arabic</td>\n      <td>/kaggle/input/preprocess-common-language/proce...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.756316</td>\n      <td>0.046051</td>\n      <td>3274.178654</td>\n      <td>2196.474265</td>\n      <td>5870.572917</td>\n      <td>0.353054</td>\n      <td>-233.02031</td>\n      <td>22.691550</td>\n      <td>10.057923</td>\n      <td>3.829097</td>\n      <td>...</td>\n      <td>-1.657243</td>\n      <td>-5.409642</td>\n      <td>-4.017134</td>\n      <td>-6.744406</td>\n      <td>-1.697630</td>\n      <td>-0.387302</td>\n      <td>0.829549</td>\n      <td>1.292110</td>\n      <td>Arabic</td>\n      <td>/kaggle/input/preprocess-common-language/proce...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.588983</td>\n      <td>0.061376</td>\n      <td>1948.418292</td>\n      <td>2049.242741</td>\n      <td>4186.921296</td>\n      <td>0.137682</td>\n      <td>-199.04490</td>\n      <td>80.806870</td>\n      <td>31.451380</td>\n      <td>-1.297673</td>\n      <td>...</td>\n      <td>-4.202263</td>\n      <td>0.065943</td>\n      <td>-9.312079</td>\n      <td>-7.164060</td>\n      <td>-3.082040</td>\n      <td>-8.046175</td>\n      <td>-3.083879</td>\n      <td>-2.018449</td>\n      <td>Arabic</td>\n      <td>/kaggle/input/preprocess-common-language/proce...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.647222</td>\n      <td>0.069638</td>\n      <td>1705.618989</td>\n      <td>1824.714129</td>\n      <td>3730.709877</td>\n      <td>0.095540</td>\n      <td>-325.47556</td>\n      <td>92.373820</td>\n      <td>17.725632</td>\n      <td>31.867613</td>\n      <td>...</td>\n      <td>3.169903</td>\n      <td>4.538502</td>\n      <td>-1.073114</td>\n      <td>-1.204524</td>\n      <td>-0.108214</td>\n      <td>-4.803460</td>\n      <td>-2.882802</td>\n      <td>-1.455632</td>\n      <td>Arabic</td>\n      <td>/kaggle/input/preprocess-common-language/proce...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 28 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T11:45:45.690329Z","iopub.execute_input":"2024-07-06T11:45:45.690693Z","iopub.status.idle":"2024-07-06T11:45:45.930146Z","shell.execute_reply.started":"2024-07-06T11:45:45.690665Z","shell.execute_reply":"2024-07-06T11:45:45.929020Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Initialize LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Fit LabelEncoder with your actual labels\nlabel_encoder.fit(df['label'])\n\n# Transform actual labels to numeric labels\ndf['numeric_labels'] = label_encoder.transform(df['label'])","metadata":{"execution":{"iopub.status.busy":"2024-07-06T11:45:45.931509Z","iopub.execute_input":"2024-07-06T11:45:45.931914Z","iopub.status.idle":"2024-07-06T11:45:45.958826Z","shell.execute_reply.started":"2024-07-06T11:45:45.931878Z","shell.execute_reply":"2024-07-06T11:45:45.957574Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"X = df.drop(columns=['label','numeric_labels','file_path'])\ny = df['numeric_labels']","metadata":{"execution":{"iopub.status.busy":"2024-07-06T11:45:45.960233Z","iopub.execute_input":"2024-07-06T11:45:45.960669Z","iopub.status.idle":"2024-07-06T11:45:45.980351Z","shell.execute_reply.started":"2024-07-06T11:45:45.960630Z","shell.execute_reply":"2024-07-06T11:45:45.979133Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(sampling_strategy='auto', random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X, y)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T11:45:45.981746Z","iopub.execute_input":"2024-07-06T11:45:45.982187Z","iopub.status.idle":"2024-07-06T11:45:47.208939Z","shell.execute_reply.started":"2024-07-06T11:45:45.982149Z","shell.execute_reply":"2024-07-06T11:45:47.207672Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\n\n# Jumlah kelas sebelum SMOTE\nprint(\"Jumlah kelas sebelum SMOTE:\")\nprint(Counter(y))\n\n# Jumlah kelas setelah SMOTE\nprint(\"\\nJumlah kelas setelah SMOTE:\")\nprint(Counter(y_resampled))","metadata":{"execution":{"iopub.status.busy":"2024-07-06T11:45:47.210341Z","iopub.execute_input":"2024-07-06T11:45:47.210944Z","iopub.status.idle":"2024-07-06T11:45:47.254202Z","shell.execute_reply.started":"2024-07-06T11:45:47.210900Z","shell.execute_reply":"2024-07-06T11:45:47.252850Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Jumlah kelas sebelum SMOTE:\nCounter({2: 3570, 39: 3177, 19: 3012, 6: 3003, 27: 2955, 41: 2784, 24: 2766, 0: 2751, 20: 2733, 37: 2691, 10: 2550, 33: 2547, 18: 2520, 21: 2484, 42: 2472, 8: 2442, 40: 2367, 26: 2331, 30: 2283, 28: 2277, 23: 2268, 32: 2238, 7: 2202, 31: 2196, 43: 2181, 15: 2073, 44: 2073, 5: 2016, 34: 1977, 9: 1962, 25: 1947, 12: 1932, 1: 1914, 16: 1890, 29: 1884, 14: 1863, 3: 1830, 17: 1830, 4: 1797, 11: 1773, 35: 1758, 36: 1752, 38: 1737, 22: 1695, 13: 1623})\n\nJumlah kelas setelah SMOTE:\nCounter({0: 3570, 4: 3570, 9: 3570, 27: 3570, 18: 3570, 24: 3570, 23: 3570, 13: 3570, 20: 3570, 11: 3570, 16: 3570, 40: 3570, 28: 3570, 15: 3570, 21: 3570, 8: 3570, 31: 3570, 29: 3570, 5: 3570, 7: 3570, 41: 3570, 44: 3570, 3: 3570, 2: 3570, 14: 3570, 43: 3570, 36: 3570, 26: 3570, 34: 3570, 12: 3570, 37: 3570, 35: 3570, 42: 3570, 33: 3570, 17: 3570, 25: 3570, 6: 3570, 30: 3570, 39: 3570, 22: 3570, 10: 3570, 1: 3570, 19: 3570, 32: 3570, 38: 3570})\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled,\n                                                    test_size=0.15, random_state=42,\n                                                    stratify=y_resampled)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T11:45:47.256209Z","iopub.execute_input":"2024-07-06T11:45:47.256725Z","iopub.status.idle":"2024-07-06T11:45:47.424597Z","shell.execute_reply.started":"2024-07-06T11:45:47.256688Z","shell.execute_reply":"2024-07-06T11:45:47.423609Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Inisialisasi StandardScaler\nscaler = StandardScaler()\n\n# Fit scaler pada data training dan transform kedua set data\nX_train_normalized = scaler.fit_transform(X_train)\nX_test_normalized = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T11:45:47.426010Z","iopub.execute_input":"2024-07-06T11:45:47.426451Z","iopub.status.idle":"2024-07-06T11:45:47.515793Z","shell.execute_reply.started":"2024-07-06T11:45:47.426418Z","shell.execute_reply":"2024-07-06T11:45:47.514740Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(X_train_normalized)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T11:45:47.517115Z","iopub.execute_input":"2024-07-06T11:45:47.517468Z","iopub.status.idle":"2024-07-06T11:45:47.555687Z","shell.execute_reply.started":"2024-07-06T11:45:47.517438Z","shell.execute_reply":"2024-07-06T11:45:47.554539Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"              0         1         2         3         4         5         6   \\\n0       0.582034 -0.406956 -0.531578  0.290200 -0.229217 -0.810377  0.751004   \n1       1.460424 -0.838015  1.086674  1.006211  1.027872  0.961832 -0.119324   \n2       0.053463  1.206179 -1.425506 -0.800742 -1.422655 -1.136345 -0.735763   \n3       0.429476  0.161088  0.280372  0.327998  0.306816  0.215003  1.401919   \n4      -0.766364 -0.418856  0.040892  0.673203  0.374364 -0.422581  0.370732   \n...          ...       ...       ...       ...       ...       ...       ...   \n136547 -0.326621 -0.363583  0.890655  0.978043  1.090333  0.705216  0.739100   \n136548  0.145052 -1.101250  0.583430  0.723028  0.744013  0.441511 -0.151909   \n136549 -2.113605 -0.751136 -0.196731  0.967075  0.588841 -1.066331 -0.223444   \n136550  0.474722  0.917518 -0.078406  0.201561  0.228375 -0.644907  0.623080   \n136551  0.008309 -1.433016  0.243556  0.952737  0.694704 -0.494726 -0.382777   \n\n              7         8         9   ...        16        17        18  \\\n0       0.309937  0.150633  0.245647  ... -0.954706 -1.645642 -0.738238   \n1      -0.695537  0.713919 -0.193797  ...  1.591395  0.035205  0.895463   \n2       1.107606  0.744095  1.590166  ...  0.900279  0.503818 -0.335897   \n3      -0.276785 -0.403517  0.347474  ...  0.089280 -0.362908 -0.171992   \n4      -0.317502  0.521316 -0.223565  ... -0.031470 -0.213825 -0.061401   \n...          ...       ...       ...  ...       ...       ...       ...   \n136547 -0.741826 -0.068819 -0.613114  ... -0.239633  0.211900 -0.334186   \n136548 -0.666811  0.014441 -0.297896  ... -0.192679  0.010025  0.101929   \n136549 -0.199056  0.923485  0.074654  ...  0.137754  0.039006  0.383723   \n136550 -0.541088 -0.380098  1.282401  ...  0.323344  2.360557  1.968953   \n136551 -0.385765  0.895022  0.327955  ... -0.076508 -0.632286  0.177623   \n\n              19        20        21        22        23        24        25  \n0      -0.625199 -0.328946 -1.619852 -1.118937 -0.381337 -0.036970 -0.709218  \n1       0.519906  0.952624  0.910742  0.839988  0.347857  1.085478  1.424219  \n2      -0.575068  0.071210  0.356524  0.366233  0.547966  0.021219  0.455161  \n3       0.350655 -0.017171  0.420310  0.590368 -0.279936  0.517072  0.032433  \n4      -0.893875 -0.128419 -0.636235 -1.691928 -2.193984 -0.690640 -1.756480  \n...          ...       ...       ...       ...       ...       ...       ...  \n136547 -1.044798  0.019457 -0.685120 -0.554165  0.609366 -0.652576 -0.150498  \n136548 -0.745057  0.451718  1.320971  0.494942  1.124170  1.218560 -0.209068  \n136549 -0.246038  0.538013 -0.178155 -0.104413 -1.023905 -1.373848 -1.612716  \n136550  0.935470 -0.310959  2.269708  1.095870  0.035977  0.673887  1.690333  \n136551  0.657724  1.558991  1.070492  1.447310  1.468942  1.242074  2.111769  \n\n[136552 rows x 26 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.582034</td>\n      <td>-0.406956</td>\n      <td>-0.531578</td>\n      <td>0.290200</td>\n      <td>-0.229217</td>\n      <td>-0.810377</td>\n      <td>0.751004</td>\n      <td>0.309937</td>\n      <td>0.150633</td>\n      <td>0.245647</td>\n      <td>...</td>\n      <td>-0.954706</td>\n      <td>-1.645642</td>\n      <td>-0.738238</td>\n      <td>-0.625199</td>\n      <td>-0.328946</td>\n      <td>-1.619852</td>\n      <td>-1.118937</td>\n      <td>-0.381337</td>\n      <td>-0.036970</td>\n      <td>-0.709218</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.460424</td>\n      <td>-0.838015</td>\n      <td>1.086674</td>\n      <td>1.006211</td>\n      <td>1.027872</td>\n      <td>0.961832</td>\n      <td>-0.119324</td>\n      <td>-0.695537</td>\n      <td>0.713919</td>\n      <td>-0.193797</td>\n      <td>...</td>\n      <td>1.591395</td>\n      <td>0.035205</td>\n      <td>0.895463</td>\n      <td>0.519906</td>\n      <td>0.952624</td>\n      <td>0.910742</td>\n      <td>0.839988</td>\n      <td>0.347857</td>\n      <td>1.085478</td>\n      <td>1.424219</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.053463</td>\n      <td>1.206179</td>\n      <td>-1.425506</td>\n      <td>-0.800742</td>\n      <td>-1.422655</td>\n      <td>-1.136345</td>\n      <td>-0.735763</td>\n      <td>1.107606</td>\n      <td>0.744095</td>\n      <td>1.590166</td>\n      <td>...</td>\n      <td>0.900279</td>\n      <td>0.503818</td>\n      <td>-0.335897</td>\n      <td>-0.575068</td>\n      <td>0.071210</td>\n      <td>0.356524</td>\n      <td>0.366233</td>\n      <td>0.547966</td>\n      <td>0.021219</td>\n      <td>0.455161</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.429476</td>\n      <td>0.161088</td>\n      <td>0.280372</td>\n      <td>0.327998</td>\n      <td>0.306816</td>\n      <td>0.215003</td>\n      <td>1.401919</td>\n      <td>-0.276785</td>\n      <td>-0.403517</td>\n      <td>0.347474</td>\n      <td>...</td>\n      <td>0.089280</td>\n      <td>-0.362908</td>\n      <td>-0.171992</td>\n      <td>0.350655</td>\n      <td>-0.017171</td>\n      <td>0.420310</td>\n      <td>0.590368</td>\n      <td>-0.279936</td>\n      <td>0.517072</td>\n      <td>0.032433</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.766364</td>\n      <td>-0.418856</td>\n      <td>0.040892</td>\n      <td>0.673203</td>\n      <td>0.374364</td>\n      <td>-0.422581</td>\n      <td>0.370732</td>\n      <td>-0.317502</td>\n      <td>0.521316</td>\n      <td>-0.223565</td>\n      <td>...</td>\n      <td>-0.031470</td>\n      <td>-0.213825</td>\n      <td>-0.061401</td>\n      <td>-0.893875</td>\n      <td>-0.128419</td>\n      <td>-0.636235</td>\n      <td>-1.691928</td>\n      <td>-2.193984</td>\n      <td>-0.690640</td>\n      <td>-1.756480</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>136547</th>\n      <td>-0.326621</td>\n      <td>-0.363583</td>\n      <td>0.890655</td>\n      <td>0.978043</td>\n      <td>1.090333</td>\n      <td>0.705216</td>\n      <td>0.739100</td>\n      <td>-0.741826</td>\n      <td>-0.068819</td>\n      <td>-0.613114</td>\n      <td>...</td>\n      <td>-0.239633</td>\n      <td>0.211900</td>\n      <td>-0.334186</td>\n      <td>-1.044798</td>\n      <td>0.019457</td>\n      <td>-0.685120</td>\n      <td>-0.554165</td>\n      <td>0.609366</td>\n      <td>-0.652576</td>\n      <td>-0.150498</td>\n    </tr>\n    <tr>\n      <th>136548</th>\n      <td>0.145052</td>\n      <td>-1.101250</td>\n      <td>0.583430</td>\n      <td>0.723028</td>\n      <td>0.744013</td>\n      <td>0.441511</td>\n      <td>-0.151909</td>\n      <td>-0.666811</td>\n      <td>0.014441</td>\n      <td>-0.297896</td>\n      <td>...</td>\n      <td>-0.192679</td>\n      <td>0.010025</td>\n      <td>0.101929</td>\n      <td>-0.745057</td>\n      <td>0.451718</td>\n      <td>1.320971</td>\n      <td>0.494942</td>\n      <td>1.124170</td>\n      <td>1.218560</td>\n      <td>-0.209068</td>\n    </tr>\n    <tr>\n      <th>136549</th>\n      <td>-2.113605</td>\n      <td>-0.751136</td>\n      <td>-0.196731</td>\n      <td>0.967075</td>\n      <td>0.588841</td>\n      <td>-1.066331</td>\n      <td>-0.223444</td>\n      <td>-0.199056</td>\n      <td>0.923485</td>\n      <td>0.074654</td>\n      <td>...</td>\n      <td>0.137754</td>\n      <td>0.039006</td>\n      <td>0.383723</td>\n      <td>-0.246038</td>\n      <td>0.538013</td>\n      <td>-0.178155</td>\n      <td>-0.104413</td>\n      <td>-1.023905</td>\n      <td>-1.373848</td>\n      <td>-1.612716</td>\n    </tr>\n    <tr>\n      <th>136550</th>\n      <td>0.474722</td>\n      <td>0.917518</td>\n      <td>-0.078406</td>\n      <td>0.201561</td>\n      <td>0.228375</td>\n      <td>-0.644907</td>\n      <td>0.623080</td>\n      <td>-0.541088</td>\n      <td>-0.380098</td>\n      <td>1.282401</td>\n      <td>...</td>\n      <td>0.323344</td>\n      <td>2.360557</td>\n      <td>1.968953</td>\n      <td>0.935470</td>\n      <td>-0.310959</td>\n      <td>2.269708</td>\n      <td>1.095870</td>\n      <td>0.035977</td>\n      <td>0.673887</td>\n      <td>1.690333</td>\n    </tr>\n    <tr>\n      <th>136551</th>\n      <td>0.008309</td>\n      <td>-1.433016</td>\n      <td>0.243556</td>\n      <td>0.952737</td>\n      <td>0.694704</td>\n      <td>-0.494726</td>\n      <td>-0.382777</td>\n      <td>-0.385765</td>\n      <td>0.895022</td>\n      <td>0.327955</td>\n      <td>...</td>\n      <td>-0.076508</td>\n      <td>-0.632286</td>\n      <td>0.177623</td>\n      <td>0.657724</td>\n      <td>1.558991</td>\n      <td>1.070492</td>\n      <td>1.447310</td>\n      <td>1.468942</td>\n      <td>1.242074</td>\n      <td>2.111769</td>\n    </tr>\n  </tbody>\n</table>\n<p>136552 rows Ã— 26 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\nnp.random.seed(42)\n\n# Base models\nrf = RandomForestClassifier(\n    n_estimators=200,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    max_features='log2',\n    max_depth=None,\n    bootstrap=False,\n    random_state=42\n)\nknn = KNeighborsClassifier(\n    n_neighbors=1)\n\ndt = DecisionTreeClassifier(\n\trandom_state=42)\n\n# Final estimator\nfinal_estimator = LogisticRegression(multi_class='multinomial', max_iter=2000)\n\n# Function to get out-of-fold predictions\ndef get_oof_pred(model, X, y, cv=3):\n    oof_pred = cross_val_predict(model, X, y, cv=cv, method='predict_proba')\n    return oof_pred\n\n# Get out-of-fold predictions for base models\nrf_oof = get_oof_pred(rf, X_train_normalized, y_train)\nknn_oof = get_oof_pred(knn, X_train_normalized, y_train)\ndt_oof = get_oof_pred(dt, X_train_normalized, y_train)\n\n# Combine out-of-fold predictions\nX_train_meta = np.hstack([rf_oof, knn_oof, dt_oof])\n\n# Fit final estimator\nfinal_estimator.fit(X_train_meta, y_train)\n\n# Fit base models on entire training data\nrf.fit(X_train_normalized, y_train)\nknn.fit(X_train_normalized, y_train)\ndt.fit(X_train_normalized, y_train)\n\n# Get predictions for test data\nrf_test = rf.predict_proba(X_test_normalized)\nknn_test = knn.predict_proba(X_test_normalized)\ndt_test = dt.predict_proba(X_test_normalized)\n\n# Combine test predictions\nX_test_meta = np.hstack([rf_test, knn_test, dt_test])\n\n# Final prediction\nfinal_pred = final_estimator.predict(X_test_meta)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, final_pred)\nprint(f\"Accuracy of the stacking model: {accuracy:.5f}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-06T11:45:47.557159Z","iopub.execute_input":"2024-07-06T11:45:47.557557Z","iopub.status.idle":"2024-07-06T12:14:23.811294Z","shell.execute_reply.started":"2024-07-06T11:45:47.557526Z","shell.execute_reply":"2024-07-06T12:14:23.803958Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Accuracy of the stacking model: 0.88514\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report\n\n# Generate and print classification report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, final_pred))","metadata":{"execution":{"iopub.status.busy":"2024-07-06T12:14:23.817027Z","iopub.execute_input":"2024-07-06T12:14:23.817564Z","iopub.status.idle":"2024-07-06T12:14:23.932606Z","shell.execute_reply.started":"2024-07-06T12:14:23.817523Z","shell.execute_reply":"2024-07-06T12:14:23.931419Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.91      0.82      0.86       536\n           1       0.86      0.88      0.87       536\n           2       0.91      0.88      0.89       536\n           3       0.84      0.87      0.85       535\n           4       0.88      0.86      0.87       535\n           5       0.89      0.86      0.87       536\n           6       0.86      0.74      0.80       535\n           7       0.89      0.94      0.91       536\n           8       0.89      0.87      0.88       535\n           9       0.89      0.95      0.92       535\n          10       0.88      0.82      0.85       536\n          11       0.86      0.85      0.85       536\n          12       0.86      0.87      0.86       535\n          13       0.88      0.93      0.90       535\n          14       0.90      0.86      0.88       536\n          15       0.88      0.95      0.91       535\n          16       0.94      0.96      0.95       535\n          17       0.89      0.86      0.87       535\n          18       0.88      0.92      0.90       536\n          19       0.91      0.88      0.90       535\n          20       0.94      0.91      0.92       536\n          21       0.91      0.93      0.92       536\n          22       0.86      0.89      0.88       536\n          23       0.92      0.91      0.91       536\n          24       0.89      0.78      0.83       535\n          25       0.87      0.90      0.89       535\n          26       0.90      0.90      0.90       535\n          27       0.93      0.89      0.91       536\n          28       0.92      0.93      0.92       536\n          29       0.87      0.90      0.88       536\n          30       0.86      0.80      0.83       536\n          31       0.86      0.81      0.83       535\n          32       0.82      0.82      0.82       535\n          33       0.89      0.96      0.92       536\n          34       0.87      0.96      0.91       536\n          35       0.86      0.88      0.87       535\n          36       0.88      0.96      0.92       536\n          37       0.97      0.96      0.96       535\n          38       0.85      0.88      0.87       536\n          39       0.90      0.86      0.88       535\n          40       0.88      0.93      0.90       536\n          41       0.90      0.89      0.90       535\n          42       0.88      0.84      0.86       535\n          43       0.87      0.90      0.88       535\n          44       0.87      0.87      0.87       536\n\n    accuracy                           0.89     24098\n   macro avg       0.89      0.89      0.88     24098\nweighted avg       0.89      0.89      0.88     24098\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}