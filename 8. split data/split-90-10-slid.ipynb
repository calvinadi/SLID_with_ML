{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8848655,"sourceType":"datasetVersion","datasetId":5326116}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-06T12:15:14.017662Z","iopub.execute_input":"2024-07-06T12:15:14.018196Z","iopub.status.idle":"2024-07-06T12:15:14.921589Z","shell.execute_reply.started":"2024-07-06T12:15:14.018151Z","shell.execute_reply":"2024-07-06T12:15:14.920491Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nimport joblib\nfrom sklearn.model_selection import KFold","metadata":{"execution":{"iopub.status.busy":"2024-07-06T12:15:14.922982Z","iopub.execute_input":"2024-07-06T12:15:14.923452Z","iopub.status.idle":"2024-07-06T12:15:14.928941Z","shell.execute_reply.started":"2024-07-06T12:15:14.923421Z","shell.execute_reply":"2024-07-06T12:15:14.927831Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(r\"/kaggle/input/feature-common-language/audio_features_partial.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T12:15:14.930669Z","iopub.execute_input":"2024-07-06T12:15:14.931116Z","iopub.status.idle":"2024-07-06T12:15:15.827814Z","shell.execute_reply.started":"2024-07-06T12:15:14.931076Z","shell.execute_reply":"2024-07-06T12:15:15.826711Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   chroma_stft       rms  spectral_centroid  spectral_bandwidth  \\\n0     0.712224  0.055740        2989.050985         2193.800068   \n1     0.598403  0.074004        2372.315827         2065.561020   \n2     0.756316  0.046051        3274.178654         2196.474265   \n3     0.588983  0.061376        1948.418292         2049.242741   \n4     0.647222  0.069638        1705.618989         1824.714129   \n\n   spectral_rolloff  zero_crossing_rate     mfcc_1     mfcc_2     mfcc_3  \\\n0       5666.294643            0.275144 -202.32579  27.614292   4.094784   \n1       4795.649510            0.178041 -137.41476  59.931843   5.077963   \n2       5870.572917            0.353054 -233.02031  22.691550  10.057923   \n3       4186.921296            0.137682 -199.04490  80.806870  31.451380   \n4       3730.709877            0.095540 -325.47556  92.373820  17.725632   \n\n      mfcc_4  ...    mfcc_13   mfcc_14    mfcc_15   mfcc_16   mfcc_17  \\\n0   5.301181  ...   4.752774 -0.985637  -6.752584 -4.679379 -5.478848   \n1  -5.712012  ... -12.469353 -2.134825 -10.989368 -1.460541 -4.485021   \n2   3.829097  ...  -1.657243 -5.409642  -4.017134 -6.744406 -1.697630   \n3  -1.297673  ...  -4.202263  0.065943  -9.312079 -7.164060 -3.082040   \n4  31.867613  ...   3.169903  4.538502  -1.073114 -1.204524 -0.108214   \n\n    mfcc_18   mfcc_19   mfcc_20   label  \\\n0 -0.866508 -1.919669 -0.634521  Arabic   \n1 -0.408789 -8.211143 -5.170048  Arabic   \n2 -0.387302  0.829549  1.292110  Arabic   \n3 -8.046175 -3.083879 -2.018449  Arabic   \n4 -4.803460 -2.882802 -1.455632  Arabic   \n\n                                           file_path  \n0  /kaggle/input/preprocess-common-language/proce...  \n1  /kaggle/input/preprocess-common-language/proce...  \n2  /kaggle/input/preprocess-common-language/proce...  \n3  /kaggle/input/preprocess-common-language/proce...  \n4  /kaggle/input/preprocess-common-language/proce...  \n\n[5 rows x 28 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chroma_stft</th>\n      <th>rms</th>\n      <th>spectral_centroid</th>\n      <th>spectral_bandwidth</th>\n      <th>spectral_rolloff</th>\n      <th>zero_crossing_rate</th>\n      <th>mfcc_1</th>\n      <th>mfcc_2</th>\n      <th>mfcc_3</th>\n      <th>mfcc_4</th>\n      <th>...</th>\n      <th>mfcc_13</th>\n      <th>mfcc_14</th>\n      <th>mfcc_15</th>\n      <th>mfcc_16</th>\n      <th>mfcc_17</th>\n      <th>mfcc_18</th>\n      <th>mfcc_19</th>\n      <th>mfcc_20</th>\n      <th>label</th>\n      <th>file_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.712224</td>\n      <td>0.055740</td>\n      <td>2989.050985</td>\n      <td>2193.800068</td>\n      <td>5666.294643</td>\n      <td>0.275144</td>\n      <td>-202.32579</td>\n      <td>27.614292</td>\n      <td>4.094784</td>\n      <td>5.301181</td>\n      <td>...</td>\n      <td>4.752774</td>\n      <td>-0.985637</td>\n      <td>-6.752584</td>\n      <td>-4.679379</td>\n      <td>-5.478848</td>\n      <td>-0.866508</td>\n      <td>-1.919669</td>\n      <td>-0.634521</td>\n      <td>Arabic</td>\n      <td>/kaggle/input/preprocess-common-language/proce...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.598403</td>\n      <td>0.074004</td>\n      <td>2372.315827</td>\n      <td>2065.561020</td>\n      <td>4795.649510</td>\n      <td>0.178041</td>\n      <td>-137.41476</td>\n      <td>59.931843</td>\n      <td>5.077963</td>\n      <td>-5.712012</td>\n      <td>...</td>\n      <td>-12.469353</td>\n      <td>-2.134825</td>\n      <td>-10.989368</td>\n      <td>-1.460541</td>\n      <td>-4.485021</td>\n      <td>-0.408789</td>\n      <td>-8.211143</td>\n      <td>-5.170048</td>\n      <td>Arabic</td>\n      <td>/kaggle/input/preprocess-common-language/proce...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.756316</td>\n      <td>0.046051</td>\n      <td>3274.178654</td>\n      <td>2196.474265</td>\n      <td>5870.572917</td>\n      <td>0.353054</td>\n      <td>-233.02031</td>\n      <td>22.691550</td>\n      <td>10.057923</td>\n      <td>3.829097</td>\n      <td>...</td>\n      <td>-1.657243</td>\n      <td>-5.409642</td>\n      <td>-4.017134</td>\n      <td>-6.744406</td>\n      <td>-1.697630</td>\n      <td>-0.387302</td>\n      <td>0.829549</td>\n      <td>1.292110</td>\n      <td>Arabic</td>\n      <td>/kaggle/input/preprocess-common-language/proce...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.588983</td>\n      <td>0.061376</td>\n      <td>1948.418292</td>\n      <td>2049.242741</td>\n      <td>4186.921296</td>\n      <td>0.137682</td>\n      <td>-199.04490</td>\n      <td>80.806870</td>\n      <td>31.451380</td>\n      <td>-1.297673</td>\n      <td>...</td>\n      <td>-4.202263</td>\n      <td>0.065943</td>\n      <td>-9.312079</td>\n      <td>-7.164060</td>\n      <td>-3.082040</td>\n      <td>-8.046175</td>\n      <td>-3.083879</td>\n      <td>-2.018449</td>\n      <td>Arabic</td>\n      <td>/kaggle/input/preprocess-common-language/proce...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.647222</td>\n      <td>0.069638</td>\n      <td>1705.618989</td>\n      <td>1824.714129</td>\n      <td>3730.709877</td>\n      <td>0.095540</td>\n      <td>-325.47556</td>\n      <td>92.373820</td>\n      <td>17.725632</td>\n      <td>31.867613</td>\n      <td>...</td>\n      <td>3.169903</td>\n      <td>4.538502</td>\n      <td>-1.073114</td>\n      <td>-1.204524</td>\n      <td>-0.108214</td>\n      <td>-4.803460</td>\n      <td>-2.882802</td>\n      <td>-1.455632</td>\n      <td>Arabic</td>\n      <td>/kaggle/input/preprocess-common-language/proce...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 28 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T12:15:15.830369Z","iopub.execute_input":"2024-07-06T12:15:15.830713Z","iopub.status.idle":"2024-07-06T12:15:16.071766Z","shell.execute_reply.started":"2024-07-06T12:15:15.830684Z","shell.execute_reply":"2024-07-06T12:15:16.070513Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Initialize LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Fit LabelEncoder with your actual labels\nlabel_encoder.fit(df['label'])\n\n# Transform actual labels to numeric labels\ndf['numeric_labels'] = label_encoder.transform(df['label'])","metadata":{"execution":{"iopub.status.busy":"2024-07-06T12:15:16.073126Z","iopub.execute_input":"2024-07-06T12:15:16.073549Z","iopub.status.idle":"2024-07-06T12:15:16.100646Z","shell.execute_reply.started":"2024-07-06T12:15:16.073517Z","shell.execute_reply":"2024-07-06T12:15:16.099386Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"X = df.drop(columns=['label','numeric_labels','file_path'])\ny = df['numeric_labels']","metadata":{"execution":{"iopub.status.busy":"2024-07-06T12:15:16.102085Z","iopub.execute_input":"2024-07-06T12:15:16.102495Z","iopub.status.idle":"2024-07-06T12:15:16.126146Z","shell.execute_reply.started":"2024-07-06T12:15:16.102462Z","shell.execute_reply":"2024-07-06T12:15:16.124809Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(sampling_strategy='auto', random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X, y)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T12:15:16.127866Z","iopub.execute_input":"2024-07-06T12:15:16.128359Z","iopub.status.idle":"2024-07-06T12:15:17.362538Z","shell.execute_reply.started":"2024-07-06T12:15:16.128313Z","shell.execute_reply":"2024-07-06T12:15:17.361185Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\n\n# Jumlah kelas sebelum SMOTE\nprint(\"Jumlah kelas sebelum SMOTE:\")\nprint(Counter(y))\n\n# Jumlah kelas setelah SMOTE\nprint(\"\\nJumlah kelas setelah SMOTE:\")\nprint(Counter(y_resampled))","metadata":{"execution":{"iopub.status.busy":"2024-07-06T12:15:17.364044Z","iopub.execute_input":"2024-07-06T12:15:17.364517Z","iopub.status.idle":"2024-07-06T12:15:17.407009Z","shell.execute_reply.started":"2024-07-06T12:15:17.364487Z","shell.execute_reply":"2024-07-06T12:15:17.405634Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Jumlah kelas sebelum SMOTE:\nCounter({2: 3570, 39: 3177, 19: 3012, 6: 3003, 27: 2955, 41: 2784, 24: 2766, 0: 2751, 20: 2733, 37: 2691, 10: 2550, 33: 2547, 18: 2520, 21: 2484, 42: 2472, 8: 2442, 40: 2367, 26: 2331, 30: 2283, 28: 2277, 23: 2268, 32: 2238, 7: 2202, 31: 2196, 43: 2181, 15: 2073, 44: 2073, 5: 2016, 34: 1977, 9: 1962, 25: 1947, 12: 1932, 1: 1914, 16: 1890, 29: 1884, 14: 1863, 3: 1830, 17: 1830, 4: 1797, 11: 1773, 35: 1758, 36: 1752, 38: 1737, 22: 1695, 13: 1623})\n\nJumlah kelas setelah SMOTE:\nCounter({0: 3570, 4: 3570, 9: 3570, 27: 3570, 18: 3570, 24: 3570, 23: 3570, 13: 3570, 20: 3570, 11: 3570, 16: 3570, 40: 3570, 28: 3570, 15: 3570, 21: 3570, 8: 3570, 31: 3570, 29: 3570, 5: 3570, 7: 3570, 41: 3570, 44: 3570, 3: 3570, 2: 3570, 14: 3570, 43: 3570, 36: 3570, 26: 3570, 34: 3570, 12: 3570, 37: 3570, 35: 3570, 42: 3570, 33: 3570, 17: 3570, 25: 3570, 6: 3570, 30: 3570, 39: 3570, 22: 3570, 10: 3570, 1: 3570, 19: 3570, 32: 3570, 38: 3570})\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled,\n                                                    test_size=0.1, random_state=42,\n                                                    stratify=y_resampled)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T12:15:17.408418Z","iopub.execute_input":"2024-07-06T12:15:17.408811Z","iopub.status.idle":"2024-07-06T12:15:17.539656Z","shell.execute_reply.started":"2024-07-06T12:15:17.408770Z","shell.execute_reply":"2024-07-06T12:15:17.538530Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Inisialisasi StandardScaler\nscaler = StandardScaler()\n\n# Fit scaler pada data training dan transform kedua set data\nX_train_normalized = scaler.fit_transform(X_train)\nX_test_normalized = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T12:15:17.540981Z","iopub.execute_input":"2024-07-06T12:15:17.541326Z","iopub.status.idle":"2024-07-06T12:15:17.632562Z","shell.execute_reply.started":"2024-07-06T12:15:17.541297Z","shell.execute_reply":"2024-07-06T12:15:17.631076Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(X_train_normalized)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T12:15:17.634184Z","iopub.execute_input":"2024-07-06T12:15:17.634631Z","iopub.status.idle":"2024-07-06T12:15:17.673544Z","shell.execute_reply.started":"2024-07-06T12:15:17.634578Z","shell.execute_reply":"2024-07-06T12:15:17.672477Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"              0         1         2         3         4         5         6   \\\n0       1.352238 -0.196662  0.640072 -0.284901  0.150039  0.943252  0.880910   \n1       0.271733 -0.665258  0.836996  0.396457  0.582783  0.882425  0.794394   \n2       0.528691 -0.644129  0.929076  1.055296  1.109258  0.378360 -0.140977   \n3       0.512735 -0.256270 -0.857106  0.105482 -0.708300 -0.902103  0.527255   \n4      -0.625378 -0.666065  0.561865  0.896232  0.809715  0.270311 -0.059167   \n...          ...       ...       ...       ...       ...       ...       ...   \n144580 -0.037378 -1.007166  0.597109  0.355742  0.393019  0.707309  0.319089   \n144581 -2.002882  0.713715 -2.152560 -2.585330 -2.484801 -1.354048 -2.309304   \n144582 -1.158550  0.150183 -0.425355  0.073529 -0.252905 -0.507049  0.858439   \n144583 -1.487249  0.340728 -0.733062  0.409354 -0.158977 -1.183018  0.303486   \n144584  1.198251 -1.467233  1.518858  0.894167  1.249489  1.986375 -0.861139   \n\n              7         8         9   ...        16        17        18  \\\n0      -0.821538 -1.272965  1.999582  ...  0.522169  1.053469 -0.129764   \n1      -0.673104 -0.867479 -0.794859  ...  1.522205 -1.406470 -0.187701   \n2      -1.046839  1.146569  0.340942  ...  0.194853  1.166748  0.519591   \n3       0.835797  0.851341 -0.771835  ...  1.405573  0.116354 -0.656836   \n4      -0.685461  0.501324 -0.422861  ... -0.077567 -0.842436 -0.850719   \n...          ...       ...       ...  ...       ...       ...       ...   \n144580 -0.579387 -0.870306 -1.013493  ...  0.074152  0.581089  0.705050   \n144581  1.718162 -0.822125 -0.681816  ... -1.082210 -2.144068 -0.527631   \n144582  0.128575 -0.694644 -0.802791  ... -1.886316 -1.573437  0.208306   \n144583  0.203856  0.635147 -0.210467  ...  0.709369  0.631290  0.098589   \n144584 -1.170402  0.506655 -0.372899  ...  0.706641  0.148872  0.049075   \n\n              19        20        21        22        23        24        25  \n0       1.387112 -0.021383 -1.618474  1.568124  0.056370  1.628812  1.043047  \n1       1.931407 -0.162103 -1.784205 -0.344551 -1.063729 -1.409148 -0.204198  \n2       0.958469  1.159747  0.985061  1.611397  0.022832  0.570222  1.189172  \n3      -0.675212  0.942161 -0.064677  0.887759  1.558140  0.888050  1.175272  \n4      -1.158440 -0.753182  0.093594  0.976191  1.429651  0.098162 -1.441473  \n...          ...       ...       ...       ...       ...       ...       ...  \n144580 -0.319923  0.424395  0.154773 -0.444200  0.427482  0.071098  0.029846  \n144581 -0.414229 -1.435923 -2.879149 -0.224998  1.361394  0.811245 -0.053964  \n144582 -2.257273 -1.005788  0.755442 -1.261920  0.448821  1.568874 -0.963251  \n144583  0.015874  0.338881 -0.412642 -0.521079 -0.685446  0.336596 -1.377381  \n144584 -0.115451  0.200423 -0.286944  0.398880  0.012311  0.708612  0.493389  \n\n[144585 rows x 26 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.352238</td>\n      <td>-0.196662</td>\n      <td>0.640072</td>\n      <td>-0.284901</td>\n      <td>0.150039</td>\n      <td>0.943252</td>\n      <td>0.880910</td>\n      <td>-0.821538</td>\n      <td>-1.272965</td>\n      <td>1.999582</td>\n      <td>...</td>\n      <td>0.522169</td>\n      <td>1.053469</td>\n      <td>-0.129764</td>\n      <td>1.387112</td>\n      <td>-0.021383</td>\n      <td>-1.618474</td>\n      <td>1.568124</td>\n      <td>0.056370</td>\n      <td>1.628812</td>\n      <td>1.043047</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.271733</td>\n      <td>-0.665258</td>\n      <td>0.836996</td>\n      <td>0.396457</td>\n      <td>0.582783</td>\n      <td>0.882425</td>\n      <td>0.794394</td>\n      <td>-0.673104</td>\n      <td>-0.867479</td>\n      <td>-0.794859</td>\n      <td>...</td>\n      <td>1.522205</td>\n      <td>-1.406470</td>\n      <td>-0.187701</td>\n      <td>1.931407</td>\n      <td>-0.162103</td>\n      <td>-1.784205</td>\n      <td>-0.344551</td>\n      <td>-1.063729</td>\n      <td>-1.409148</td>\n      <td>-0.204198</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.528691</td>\n      <td>-0.644129</td>\n      <td>0.929076</td>\n      <td>1.055296</td>\n      <td>1.109258</td>\n      <td>0.378360</td>\n      <td>-0.140977</td>\n      <td>-1.046839</td>\n      <td>1.146569</td>\n      <td>0.340942</td>\n      <td>...</td>\n      <td>0.194853</td>\n      <td>1.166748</td>\n      <td>0.519591</td>\n      <td>0.958469</td>\n      <td>1.159747</td>\n      <td>0.985061</td>\n      <td>1.611397</td>\n      <td>0.022832</td>\n      <td>0.570222</td>\n      <td>1.189172</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.512735</td>\n      <td>-0.256270</td>\n      <td>-0.857106</td>\n      <td>0.105482</td>\n      <td>-0.708300</td>\n      <td>-0.902103</td>\n      <td>0.527255</td>\n      <td>0.835797</td>\n      <td>0.851341</td>\n      <td>-0.771835</td>\n      <td>...</td>\n      <td>1.405573</td>\n      <td>0.116354</td>\n      <td>-0.656836</td>\n      <td>-0.675212</td>\n      <td>0.942161</td>\n      <td>-0.064677</td>\n      <td>0.887759</td>\n      <td>1.558140</td>\n      <td>0.888050</td>\n      <td>1.175272</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.625378</td>\n      <td>-0.666065</td>\n      <td>0.561865</td>\n      <td>0.896232</td>\n      <td>0.809715</td>\n      <td>0.270311</td>\n      <td>-0.059167</td>\n      <td>-0.685461</td>\n      <td>0.501324</td>\n      <td>-0.422861</td>\n      <td>...</td>\n      <td>-0.077567</td>\n      <td>-0.842436</td>\n      <td>-0.850719</td>\n      <td>-1.158440</td>\n      <td>-0.753182</td>\n      <td>0.093594</td>\n      <td>0.976191</td>\n      <td>1.429651</td>\n      <td>0.098162</td>\n      <td>-1.441473</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>144580</th>\n      <td>-0.037378</td>\n      <td>-1.007166</td>\n      <td>0.597109</td>\n      <td>0.355742</td>\n      <td>0.393019</td>\n      <td>0.707309</td>\n      <td>0.319089</td>\n      <td>-0.579387</td>\n      <td>-0.870306</td>\n      <td>-1.013493</td>\n      <td>...</td>\n      <td>0.074152</td>\n      <td>0.581089</td>\n      <td>0.705050</td>\n      <td>-0.319923</td>\n      <td>0.424395</td>\n      <td>0.154773</td>\n      <td>-0.444200</td>\n      <td>0.427482</td>\n      <td>0.071098</td>\n      <td>0.029846</td>\n    </tr>\n    <tr>\n      <th>144581</th>\n      <td>-2.002882</td>\n      <td>0.713715</td>\n      <td>-2.152560</td>\n      <td>-2.585330</td>\n      <td>-2.484801</td>\n      <td>-1.354048</td>\n      <td>-2.309304</td>\n      <td>1.718162</td>\n      <td>-0.822125</td>\n      <td>-0.681816</td>\n      <td>...</td>\n      <td>-1.082210</td>\n      <td>-2.144068</td>\n      <td>-0.527631</td>\n      <td>-0.414229</td>\n      <td>-1.435923</td>\n      <td>-2.879149</td>\n      <td>-0.224998</td>\n      <td>1.361394</td>\n      <td>0.811245</td>\n      <td>-0.053964</td>\n    </tr>\n    <tr>\n      <th>144582</th>\n      <td>-1.158550</td>\n      <td>0.150183</td>\n      <td>-0.425355</td>\n      <td>0.073529</td>\n      <td>-0.252905</td>\n      <td>-0.507049</td>\n      <td>0.858439</td>\n      <td>0.128575</td>\n      <td>-0.694644</td>\n      <td>-0.802791</td>\n      <td>...</td>\n      <td>-1.886316</td>\n      <td>-1.573437</td>\n      <td>0.208306</td>\n      <td>-2.257273</td>\n      <td>-1.005788</td>\n      <td>0.755442</td>\n      <td>-1.261920</td>\n      <td>0.448821</td>\n      <td>1.568874</td>\n      <td>-0.963251</td>\n    </tr>\n    <tr>\n      <th>144583</th>\n      <td>-1.487249</td>\n      <td>0.340728</td>\n      <td>-0.733062</td>\n      <td>0.409354</td>\n      <td>-0.158977</td>\n      <td>-1.183018</td>\n      <td>0.303486</td>\n      <td>0.203856</td>\n      <td>0.635147</td>\n      <td>-0.210467</td>\n      <td>...</td>\n      <td>0.709369</td>\n      <td>0.631290</td>\n      <td>0.098589</td>\n      <td>0.015874</td>\n      <td>0.338881</td>\n      <td>-0.412642</td>\n      <td>-0.521079</td>\n      <td>-0.685446</td>\n      <td>0.336596</td>\n      <td>-1.377381</td>\n    </tr>\n    <tr>\n      <th>144584</th>\n      <td>1.198251</td>\n      <td>-1.467233</td>\n      <td>1.518858</td>\n      <td>0.894167</td>\n      <td>1.249489</td>\n      <td>1.986375</td>\n      <td>-0.861139</td>\n      <td>-1.170402</td>\n      <td>0.506655</td>\n      <td>-0.372899</td>\n      <td>...</td>\n      <td>0.706641</td>\n      <td>0.148872</td>\n      <td>0.049075</td>\n      <td>-0.115451</td>\n      <td>0.200423</td>\n      <td>-0.286944</td>\n      <td>0.398880</td>\n      <td>0.012311</td>\n      <td>0.708612</td>\n      <td>0.493389</td>\n    </tr>\n  </tbody>\n</table>\n<p>144585 rows × 26 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\nnp.random.seed(42)\n\n# Base models\nrf = RandomForestClassifier(\n    n_estimators=200,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    max_features='log2',\n    max_depth=None,\n    bootstrap=False,\n    random_state=42\n)\nknn = KNeighborsClassifier(\n    n_neighbors=1)\n\ndt = DecisionTreeClassifier(\n\trandom_state=42)\n\n# Final estimator\nfinal_estimator = LogisticRegression(multi_class='multinomial', max_iter=2000)\n\n# Function to get out-of-fold predictions\ndef get_oof_pred(model, X, y, cv=3):\n    oof_pred = cross_val_predict(model, X, y, cv=cv, method='predict_proba')\n    return oof_pred\n\n# Get out-of-fold predictions for base models\nrf_oof = get_oof_pred(rf, X_train_normalized, y_train)\nknn_oof = get_oof_pred(knn, X_train_normalized, y_train)\ndt_oof = get_oof_pred(dt, X_train_normalized, y_train)\n\n# Combine out-of-fold predictions\nX_train_meta = np.hstack([rf_oof, knn_oof, dt_oof])\n\n# Fit final estimator\nfinal_estimator.fit(X_train_meta, y_train)\n\n# Fit base models on entire training data\nrf.fit(X_train_normalized, y_train)\nknn.fit(X_train_normalized, y_train)\ndt.fit(X_train_normalized, y_train)\n\n# Get predictions for test data\nrf_test = rf.predict_proba(X_test_normalized)\nknn_test = knn.predict_proba(X_test_normalized)\ndt_test = dt.predict_proba(X_test_normalized)\n\n# Combine test predictions\nX_test_meta = np.hstack([rf_test, knn_test, dt_test])\n\n# Final prediction\nfinal_pred = final_estimator.predict(X_test_meta)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, final_pred)\nprint(f\"Accuracy of the stacking model: {accuracy:.5f}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-06T12:15:17.675214Z","iopub.execute_input":"2024-07-06T12:15:17.676191Z","iopub.status.idle":"2024-07-06T12:45:42.854679Z","shell.execute_reply.started":"2024-07-06T12:15:17.676147Z","shell.execute_reply":"2024-07-06T12:45:42.852254Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Accuracy of the stacking model: 0.89518\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report\n\n# Generate and print classification report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, final_pred))","metadata":{"execution":{"iopub.status.busy":"2024-07-06T12:45:42.862645Z","iopub.execute_input":"2024-07-06T12:45:42.863602Z","iopub.status.idle":"2024-07-06T12:45:42.960305Z","shell.execute_reply.started":"2024-07-06T12:45:42.863550Z","shell.execute_reply":"2024-07-06T12:45:42.959094Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.88      0.90       357\n           1       0.86      0.90      0.88       357\n           2       0.92      0.87      0.90       357\n           3       0.88      0.87      0.88       357\n           4       0.89      0.88      0.88       357\n           5       0.88      0.85      0.86       357\n           6       0.92      0.76      0.84       357\n           7       0.91      0.94      0.93       357\n           8       0.90      0.87      0.89       357\n           9       0.93      0.95      0.94       357\n          10       0.88      0.87      0.87       357\n          11       0.86      0.86      0.86       357\n          12       0.86      0.89      0.87       357\n          13       0.89      0.94      0.91       357\n          14       0.89      0.88      0.88       357\n          15       0.90      0.94      0.92       357\n          16       0.94      0.97      0.96       357\n          17       0.90      0.85      0.88       357\n          18       0.90      0.92      0.91       357\n          19       0.91      0.90      0.90       357\n          20       0.95      0.92      0.94       357\n          21       0.90      0.95      0.92       357\n          22       0.88      0.89      0.88       357\n          23       0.91      0.92      0.92       357\n          24       0.91      0.80      0.85       357\n          25       0.88      0.93      0.90       357\n          26       0.90      0.92      0.91       357\n          27       0.92      0.90      0.91       357\n          28       0.94      0.94      0.94       357\n          29       0.89      0.90      0.89       357\n          30       0.84      0.84      0.84       357\n          31       0.86      0.81      0.84       357\n          32       0.86      0.84      0.85       357\n          33       0.90      0.96      0.93       357\n          34       0.87      0.97      0.92       357\n          35       0.87      0.89      0.88       357\n          36       0.88      0.96      0.92       357\n          37       0.97      0.97      0.97       357\n          38       0.89      0.88      0.88       357\n          39       0.92      0.87      0.89       357\n          40       0.91      0.92      0.92       357\n          41       0.91      0.88      0.89       357\n          42       0.89      0.87      0.88       357\n          43       0.85      0.89      0.87       357\n          44       0.86      0.89      0.87       357\n\n    accuracy                           0.90     16065\n   macro avg       0.90      0.90      0.89     16065\nweighted avg       0.90      0.90      0.89     16065\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}